nohup: ignoring input
config files: ['configs/Kinetics/TemporalCLIP_vitb16_8x16_STAdapter_SSV2.yaml']
[12/16 23:53:39][INFO] train_net.py:  942: --Train with config:
[12/16 23:53:39][INFO] train_net.py:  943: {'AUG': {'AA_TYPE': 'rand-m7-n4-mstd0.5-inc1',
         'COLOR_JITTER': 0.4,
         'ENABLE': False,
         'GEN_MASK_LOADER': False,
         'INTERPOLATION': 'bicubic',
         'MASK_FRAMES': False,
         'MASK_RATIO': 0.0,
         'MASK_TUBE': False,
         'MASK_WINDOW_SIZE': [8, 7, 7],
         'MAX_MASK_PATCHES_PER_BLOCK': None,
         'NUM_SAMPLE': 1,
         'RE_COUNT': 1,
         'RE_MODE': 'pixel',
         'RE_PROB': 0.25,
         'RE_SPLIT': False},
 'AVA': {'ANNOTATION_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/',
         'BGR': False,
         'DETECTION_SCORE_THRESH': 0.9,
         'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv',
         'FRAME_DIR': '/mnt/fair-flash3-east/ava_trainval_frames.img/',
         'FRAME_LIST_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/',
         'FULL_TEST_ON_VAL': False,
         'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv',
         'IMG_PROC_BACKEND': 'cv2',
         'LABEL_MAP_FILE': 'ava_action_list_v2.2_for_activitynet_2019.pbtxt',
         'TEST_FORCE_FLIP': False,
         'TEST_LISTS': ['val.csv'],
         'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'],
         'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'],
         'TRAIN_LISTS': ['train_all.csv'],
         'TRAIN_PCA_JITTER_ONLY': True,
         'TRAIN_PREDICT_BOX_LISTS': [],
         'TRAIN_USE_COLOR_AUGMENTATION': False},
 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}),
 'BN': {'GLOBAL_SYNC': False,
        'NORM_TYPE': 'batchnorm',
        'NUM_BATCHES_PRECISE': 200,
        'NUM_SPLITS': 1,
        'NUM_SYNC_DEVICES': 1,
        'USE_PRECISE_STATS': False,
        'WEIGHT_DECAY': 0.0},
 'CONTRASTIVE': {'BN_MLP': False,
                 'BN_SYNC_MLP': False,
                 'DELTA_CLIPS_MAX': inf,
                 'DELTA_CLIPS_MIN': -inf,
                 'DIM': 128,
                 'INTERP_MEMORY': False,
                 'KNN_ON': True,
                 'LENGTH': 239975,
                 'LOCAL_SHUFFLE_BN': True,
                 'MEM_TYPE': '1d',
                 'MLP_DIM': 2048,
                 'MOCO_MULTI_VIEW_QUEUE': False,
                 'MOMENTUM': 0.5,
                 'MOMENTUM_ANNEALING': False,
                 'NUM_CLASSES_DOWNSTREAM': 400,
                 'NUM_MLP_LAYERS': 1,
                 'PREDICTOR_DEPTHS': [],
                 'QUEUE_LEN': 65536,
                 'SEQUENTIAL': False,
                 'SIMCLR_DIST_ON': True,
                 'SWAV_QEUE_LEN': 0,
                 'T': 0.07,
                 'TYPE': 'mem'},
 'DATA': {'COLOR_RND_GRAYSCALE': 0.0,
          'DECODING_BACKEND': 'pyav',
          'DECODING_SHORT_SIZE': 256,
          'DUMMY_LOAD': False,
          'ENSEMBLE_METHOD': 'sum',
          'IN22K_TRAINVAL': False,
          'IN22k_VAL_IN1K': '',
          'INDEX_LABEL_MAPPING_FILE': '/mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_ssv2/train_rephrased.json',
          'INPUT_CHANNEL_NUM': [3],
          'INV_UNIFORM_SAMPLE': False,
          'IN_VAL_CROP_RATIO': 0.875,
          'LOADER_CHUNK_OVERALL_SIZE': 0,
          'LOADER_CHUNK_SIZE': 0,
          'MEAN': [0.48145466, 0.4578275, 0.40821073],
          'MULTI_LABEL': False,
          'NUM_FRAMES': 8,
          'PATH_LABEL_SEPARATOR': ',',
          'PATH_PREFIX': '/mnt/SSD8T/home/huangwei/projects/FROSTER/data/ssv2/videos',
          'PATH_TO_DATA_DIR': '/mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_ssv2',
          'PATH_TO_PRELOAD_IMDB': '',
          'RANDOM_FLIP': True,
          'REVERSE_INPUT_CHANNEL': False,
          'SAMPLING_RATE': 16,
          'SKIP_ROWS': 0,
          'SSL_BLUR_SIGMA_MAX': [0.0, 2.0],
          'SSL_BLUR_SIGMA_MIN': [0.0, 0.1],
          'SSL_COLOR_BRI_CON_SAT': [0.4, 0.4, 0.4],
          'SSL_COLOR_HUE': 0.1,
          'SSL_COLOR_JITTER': False,
          'SSL_MOCOV2_AUG': False,
          'STD': [0.26862954, 0.26130258, 0.27577711],
          'TARGET_FPS': 30,
          'TEST_CROP_SIZE': 224,
          'TIME_DIFF_PROB': 0.0,
          'TRAIN_CROP_NUM_SPATIAL': 1,
          'TRAIN_CROP_NUM_TEMPORAL': 1,
          'TRAIN_CROP_SIZE': 224,
          'TRAIN_JITTER_ASPECT_RELATIVE': [],
          'TRAIN_JITTER_FPS': 0.0,
          'TRAIN_JITTER_MOTION_SHIFT': False,
          'TRAIN_JITTER_SCALES': [224, 256],
          'TRAIN_JITTER_SCALES_RELATIVE': [],
          'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229],
          'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009],
                               [-0.5808, -0.0045, -0.814],
                               [-0.5836, -0.6948, 0.4203]],
          'USE_OFFSET_SAMPLING': True},
 'DATA_LOADER': {'ENABLE_MULTI_THREAD_DECODE': False,
                 'NUM_WORKERS': 8,
                 'PIN_MEMORY': True},
 'DEMO': {'BUFFER_SIZE': 0,
          'CLIP_VIS_SIZE': 10,
          'COMMON_CLASS_NAMES': ['watch (a person)',
                                 'talk to (e.g., self, a person, a group)',
                                 'listen to (a person)',
                                 'touch (an object)',
                                 'carry/hold (an object)',
                                 'walk',
                                 'sit',
                                 'lie/sleep',
                                 'bend/bow (at the waist)'],
          'COMMON_CLASS_THRES': 0.7,
          'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml',
          'DETECTRON2_THRESH': 0.9,
          'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl',
          'DISPLAY_HEIGHT': 0,
          'DISPLAY_WIDTH': 0,
          'ENABLE': False,
          'FPS': 30,
          'GT_BOXES': '',
          'INPUT_FORMAT': 'BGR',
          'INPUT_VIDEO': '',
          'LABEL_FILE_PATH': '',
          'NUM_CLIPS_SKIP': 0,
          'NUM_VIS_INSTANCES': 2,
          'OUTPUT_FILE': '',
          'OUTPUT_FPS': -1,
          'PREDS_BOXES': '',
          'SLOWMO': 1,
          'STARTING_SECOND': 900,
          'THREAD_ENABLE': False,
          'UNCOMMON_CLASS_THRES': 0.3,
          'VIS_MODE': 'thres',
          'WEBCAM': -1},
 'DETECTION': {'ALIGNED': True,
               'ENABLE': False,
               'ROI_XFORM_RESOLUTION': 7,
               'SPATIAL_SCALE_FACTOR': 16},
 'DIST_BACKEND': 'nccl',
 'IMAGENET_SIMPLELABEL_PATH': None,
 'LOG_MODEL_INFO': True,
 'LOG_PERIOD': 10,
 'MASK': {'DECODER_DEPTH': 0,
          'DECODER_EMBED_DIM': 512,
          'DECODER_SEP_POS_EMBED': False,
          'DEC_KV_KERNEL': [],
          'DEC_KV_STRIDE': [],
          'ENABLE': False,
          'HEAD_TYPE': 'separate',
          'MAE_ON': False,
          'MAE_RND_MASK': False,
          'NORM_PRED_PIXEL': True,
          'PER_FRAME_MASKING': False,
          'PRED_HOG': False,
          'PRETRAIN_DEPTH': [15],
          'SCALE_INIT_BY_DEPTH': False,
          'TIME_STRIDE_LOSS': True},
 'MIXUP': {'ALPHA': 0.8,
           'CUTMIX_ALPHA': 1.0,
           'ENABLE': False,
           'LABEL_SMOOTH_VALUE': 0.1,
           'PROB': 1.0,
           'SWITCH_PROB': 0.5},
 'MODEL': {'ACT_CHECKPOINT': False,
           'ADAPT_FINETUNE_FACTOR': 1.0,
           'ARCH': 'vitb16',
           'CLS_LOSS_RATIO': 1.0,
           'CONTEXT_LENGTH': 77,
           'DEFAULT_FINETUNE_FACTOR': 1.0,
           'DETACH_FINAL_FC': False,
           'DISTILLATION_RATIO': 2.0,
           'DROPCONNECT_RATE': 0.0,
           'DROPOUT_RATE': 0.5,
           'ENSEMBLE_PRED': False,
           'ENSEMBLE_RAWMODEL_RATIO': 0.0,
           'EXPERT_FINETUNE_FACTOR': 1.0,
           'EXPERT_INSERT_LAYERS': [10, 11],
           'FC_INIT_STD': 0.01,
           'FINETUNE_FACTOR': 1.0,
           'FP16_ALLREDUCE': False,
           'FROZEN_BN': False,
           'HEAD_ACT': 'softmax',
           'KEEP_RAW_MODEL': True,
           'LOSS_FREQ_TYPE': 'mse',
           'LOSS_FUNC': 'soft_cross_entropy',
           'MLP_FINETUNE_FACTOR': 1.0,
           'MODEL_NAME': 'TemporalClipVideo',
           'MULTI_PATHWAY_ARCH': ['slowfast'],
           'NUM_CLASSES': 87,
           'NUM_EXPERTS': 0,
           'PROMPT_NUM': 1,
           'RAW_MODEL_DISTILLATION': True,
           'RECORD_ROUTING': False,
           'ROUTING_FINETUNE_FACTOR': 1.0,
           'ROUTING_FREQUENCE_CONSTRAIN': 0.5,
           'ROUTING_FREQ_CONS_FACTOR': 1.0,
           'ROUTING_TYPE': 'patch-level',
           'SINGLE_PATHWAY_ARCH': ['2d',
                                   'c2d',
                                   'i3d',
                                   'slow',
                                   'x3d',
                                   'mvit',
                                   'maskmvit',
                                   'vitb32',
                                   'vitb16',
                                   'vitl14'],
           'STATIC_GRAPH': False,
           'TEMPORAL_MODELING_TYPE': 'expand_temporal_view',
           'TEXT_PROMPT': False,
           'USE_CHECKPOINT': False},
 'MULTIGRID': {'BN_BASE_SIZE': 8,
               'DEFAULT_B': 0,
               'DEFAULT_S': 0,
               'DEFAULT_T': 0,
               'EPOCH_FACTOR': 1.5,
               'EVAL_FREQ': 3,
               'LONG_CYCLE': False,
               'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476),
                                      (0.5, 0.7071067811865476),
                                      (0.5, 1),
                                      (1, 1)],
               'LONG_CYCLE_SAMPLING_RATE': 0,
               'SHORT_CYCLE': False,
               'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476]},
 'MVIT': {'CLS_EMBED_ON': True,
          'DEPTH': 16,
          'DIM_MUL': [],
          'DIM_MUL_IN_ATT': False,
          'DROPOUT_RATE': 0.0,
          'DROPPATH_RATE': 0.1,
          'EMBED_DIM': 96,
          'HEAD_INIT_SCALE': 1.0,
          'HEAD_MUL': [],
          'LAYER_SCALE_INIT_VALUE': 0.0,
          'MLP_RATIO': 4.0,
          'MODE': 'conv',
          'NORM': 'layernorm',
          'NORM_STEM': False,
          'NUM_HEADS': 1,
          'PATCH_2D': False,
          'PATCH_KERNEL': [3, 7, 7],
          'PATCH_PADDING': [2, 4, 4],
          'PATCH_STRIDE': [2, 4, 4],
          'POOL_FIRST': False,
          'POOL_KVQ_KERNEL': None,
          'POOL_KV_STRIDE': [],
          'POOL_KV_STRIDE_ADAPTIVE': None,
          'POOL_Q_STRIDE': [],
          'QKV_BIAS': True,
          'REL_POS_SPATIAL': False,
          'REL_POS_TEMPORAL': False,
          'REL_POS_ZERO_INIT': False,
          'RESIDUAL_POOLING': False,
          'REV': {'BUFFER_LAYERS': [],
                  'ENABLE': False,
                  'PRE_Q_FUSION': 'avg',
                  'RESPATH_FUSE': 'concat',
                  'RES_PATH': 'conv'},
          'SEPARATE_QKV': False,
          'SEP_POS_EMBED': False,
          'USE_ABS_POS': True,
          'USE_FIXED_SINCOS_POS': False,
          'USE_MEAN_POOLING': False,
          'ZERO_DECAY_POS_CLS': True},
 'NONLOCAL': {'GROUP': [[1], [1], [1], [1]],
              'INSTANTIATION': 'dot_product',
              'LOCATION': [[[]], [[]], [[]], [[]]],
              'POOL': [[[1, 2, 2], [1, 2, 2]],
                       [[1, 2, 2], [1, 2, 2]],
                       [[1, 2, 2], [1, 2, 2]],
                       [[1, 2, 2], [1, 2, 2]]]},
 'NUM_GPUS': 8,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': '/mnt/SSD8T/home/huangwei/projects/FROSTER/checkpoints/basetraining/B2N_ssv2_froster_exp02',
 'RESNET': {'DEPTH': 50,
            'INPLACE_RELU': True,
            'NUM_BLOCK_TEMP_KERNEL': [[3], [4], [6], [3]],
            'NUM_GROUPS': 1,
            'SPATIAL_DILATIONS': [[1], [1], [1], [1]],
            'SPATIAL_STRIDES': [[1], [2], [2], [2]],
            'STRIDE_1X1': False,
            'TRANS_FUNC': 'bottleneck_transform',
            'WIDTH_PER_GROUP': 64,
            'ZERO_INIT_FINAL_BN': False,
            'ZERO_INIT_FINAL_CONV': False},
 'RNG_SEED': 0,
 'SHARD_ID': 0,
 'SLOWFAST': {'ALPHA': 8,
              'BETA_INV': 8,
              'FUSION_CONV_CHANNEL_RATIO': 2,
              'FUSION_KERNEL_SZ': 5},
 'SOLVER': {'BASE_LR': 3.33e-06,
            'BASE_LR_SCALE_NUM_SHARDS': True,
            'BETAS': (0.9, 0.999),
            'CLIP_GRAD_L2NORM': 1.0,
            'CLIP_GRAD_VAL': None,
            'COSINE_AFTER_WARMUP': True,
            'COSINE_END_LR': 3.33e-08,
            'COSINE_RESTART_EPOCH': 0,
            'DAMPENING': 0.0,
            'GAMMA': 0.1,
            'LARS_ON': False,
            'LAYER_DECAY': 1.0,
            'LRS': [],
            'LR_POLICY': 'cosine',
            'MAX_EPOCH': 12,
            'MOMENTUM': 0.9,
            'NESTEROV': True,
            'OPTIMIZING_METHOD': 'adamw',
            'STEPS': [],
            'STEP_SIZE': 1,
            'WARMUP_EPOCHS': 2.0,
            'WARMUP_FACTOR': 0.1,
            'WARMUP_START_LR': 3.33e-08,
            'WEIGHT_DECAY': 0.01,
            'ZERO_WD_1D_PARAM': True},
 'TASK': '',
 'TENSORBOARD': {'CATEGORIES_PATH': '',
                 'CLASS_NAMES_PATH': '',
                 'CONFUSION_MATRIX': {'ENABLE': False,
                                      'FIGSIZE': [8, 8],
                                      'SUBSET_PATH': ''},
                 'ENABLE': False,
                 'HISTOGRAM': {'ENABLE': False,
                               'FIGSIZE': [8, 8],
                               'SUBSET_PATH': '',
                               'TOPK': 10},
                 'LOG_DIR': '',
                 'MODEL_VIS': {'ACTIVATIONS': False,
                               'COLORMAP': 'Pastel2',
                               'ENABLE': False,
                               'GRAD_CAM': {'COLORMAP': 'viridis',
                                            'ENABLE': True,
                                            'LAYER_LIST': [],
                                            'USE_TRUE_LABEL': False},
                               'INPUT_VIDEO': False,
                               'LAYER_LIST': [],
                               'MODEL_WEIGHTS': False,
                               'TOPK_PREDS': 1},
                 'PREDICTIONS_PATH': '',
                 'WRONG_PRED_VIS': {'ENABLE': False,
                                    'SUBSET_PATH': '',
                                    'TAG': 'Incorrectly classified videos.'}},
 'TEST': {'BATCH_SIZE': 240,
          'CHECKPOINT_FILE_PATH': '',
          'CHECKPOINT_TYPE': 'pytorch',
          'CLIP_ORI_PATH': None,
          'CUSTOM_LOAD': False,
          'CUSTOM_LOAD_FILE': None,
          'DATASET': 'kinetics',
          'ENABLE': True,
          'NUM_ENSEMBLE_VIEWS': 3,
          'NUM_SPATIAL_CROPS': 1,
          'NUM_TEMPORAL_CLIPS': [],
          'OPENSET': False,
          'PATCHING_MODEL': False,
          'PATCHING_RATIO': 0.5,
          'SAVE_RESULTS_PATH': '',
          'UPDATE_STATE': False},
 'TEST_FILE': 'test.csv',
 'TRAIN': {'ADAPT_ZS_CONS_RATIO': False,
           'AUTO_RESUME': True,
           'BATCH_SIZE': 32,
           'CHECKPOINT_CLEAR_NAME_PATTERN': (),
           'CHECKPOINT_EPOCH_RESET': False,
           'CHECKPOINT_FILE_PATH': '',
           'CHECKPOINT_INFLATE': False,
           'CHECKPOINT_IN_INIT': False,
           'CHECKPOINT_PERIOD': 1,
           'CHECKPOINT_TYPE': 'pytorch',
           'CLIP_ORI_PATH': '/mnt/SSD8T/home/huangwei/.cache/clip/ViT-B-16.pt',
           'CUSTOM_LOAD': False,
           'CUSTOM_LOAD_FILE': None,
           'DATASET': 'kinetics',
           'ENABLE': True,
           'EVAL_PERIOD': 1,
           'EWC_CONSTRAIN_RATIO': 1.0,
           'EWC_IDENTITY_FISHER': False,
           'EWC_IGNORE_LOGIT_SCALE': False,
           'EWC_LOAD_FILE': None,
           'EWC_SET': False,
           'KILL_LOSS_EXPLOSION_FACTOR': 0.0,
           'LINEAR_CONNECT_CLIMB': False,
           'LINEAR_CONNECT_LOSS_RATIO': 0.0,
           'LINEAR_CONNECT_SAMPLE': True,
           'LINEAR_CONNECT_SAMPLE_L': 0.4,
           'LINEAR_CONNECT_SAMPLE_R': 0.6,
           'MIXED_PRECISION': True,
           'ZS_CONS': False,
           'ZS_CONS_RATIO': 0.8,
           'ZS_INIT_CONS': False,
           'ZS_RESTART_CONS': False,
           'ZS_RESTART_EPOCH': -1},
 'TRAIN_FILE': 'train.csv',
 'TUNE_HEAD': False,
 'VAL_FILE': 'test.csv',
 'VAL_MODE': False,
 'VIS_MASK': CfgNode({'ENABLE': False}),
 'X3D': {'BN_LIN5': False,
         'BOTTLENECK_FACTOR': 1.0,
         'CHANNELWISE_3x3x3': True,
         'DEPTH_FACTOR': 1.0,
         'DIM_C1': 12,
         'DIM_C5': 2048,
         'SCALE_RES2': False,
         'WIDTH_FACTOR': 1.0}}
[12/16 23:53:43][INFO] temporalclip_video_model.py:  657: load pretrained CLIP:<All keys matched successfully>
[12/16 23:53:51][INFO] temporalclip_video_model.py:  657: load pretrained CLIP:<All keys matched successfully>
[12/16 23:53:53][INFO] train_net.py:  951: total trainable parameters:
[12/16 23:53:53][INFO] train_net.py:  952: ['module.model.positional_embedding',
 'module.model.text_projection',
 'module.model.logit_scale',
 'module.model.visual.class_embedding',
 'module.model.visual.positional_embedding',
 'module.model.visual.proj',
 'module.model.visual.conv1.weight',
 'module.model.visual.ln_pre.weight',
 'module.model.visual.ln_pre.bias',
 'module.model.visual.transformer.resblocks.0.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.0.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.0.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.0.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.0.ln_1.weight',
 'module.model.visual.transformer.resblocks.0.ln_1.bias',
 'module.model.visual.transformer.resblocks.0.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.0.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.0.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.0.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.0.ln_2.weight',
 'module.model.visual.transformer.resblocks.0.ln_2.bias',
 'module.model.visual.transformer.resblocks.1.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.1.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.1.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.1.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.1.ln_1.weight',
 'module.model.visual.transformer.resblocks.1.ln_1.bias',
 'module.model.visual.transformer.resblocks.1.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.1.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.1.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.1.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.1.ln_2.weight',
 'module.model.visual.transformer.resblocks.1.ln_2.bias',
 'module.model.visual.transformer.resblocks.2.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.2.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.2.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.2.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.2.ln_1.weight',
 'module.model.visual.transformer.resblocks.2.ln_1.bias',
 'module.model.visual.transformer.resblocks.2.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.2.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.2.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.2.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.2.ln_2.weight',
 'module.model.visual.transformer.resblocks.2.ln_2.bias',
 'module.model.visual.transformer.resblocks.3.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.3.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.3.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.3.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.3.ln_1.weight',
 'module.model.visual.transformer.resblocks.3.ln_1.bias',
 'module.model.visual.transformer.resblocks.3.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.3.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.3.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.3.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.3.ln_2.weight',
 'module.model.visual.transformer.resblocks.3.ln_2.bias',
 'module.model.visual.transformer.resblocks.4.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.4.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.4.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.4.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.4.ln_1.weight',
 'module.model.visual.transformer.resblocks.4.ln_1.bias',
 'module.model.visual.transformer.resblocks.4.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.4.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.4.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.4.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.4.ln_2.weight',
 'module.model.visual.transformer.resblocks.4.ln_2.bias',
 'module.model.visual.transformer.resblocks.5.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.5.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.5.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.5.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.5.ln_1.weight',
 'module.model.visual.transformer.resblocks.5.ln_1.bias',
 'module.model.visual.transformer.resblocks.5.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.5.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.5.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.5.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.5.ln_2.weight',
 'module.model.visual.transformer.resblocks.5.ln_2.bias',
 'module.model.visual.transformer.resblocks.6.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.6.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.6.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.6.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.6.ln_1.weight',
 'module.model.visual.transformer.resblocks.6.ln_1.bias',
 'module.model.visual.transformer.resblocks.6.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.6.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.6.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.6.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.6.ln_2.weight',
 'module.model.visual.transformer.resblocks.6.ln_2.bias',
 'module.model.visual.transformer.resblocks.7.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.7.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.7.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.7.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.7.ln_1.weight',
 'module.model.visual.transformer.resblocks.7.ln_1.bias',
 'module.model.visual.transformer.resblocks.7.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.7.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.7.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.7.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.7.ln_2.weight',
 'module.model.visual.transformer.resblocks.7.ln_2.bias',
 'module.model.visual.transformer.resblocks.8.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.8.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.8.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.8.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.8.ln_1.weight',
 'module.model.visual.transformer.resblocks.8.ln_1.bias',
 'module.model.visual.transformer.resblocks.8.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.8.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.8.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.8.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.8.ln_2.weight',
 'module.model.visual.transformer.resblocks.8.ln_2.bias',
 'module.model.visual.transformer.resblocks.9.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.9.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.9.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.9.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.9.ln_1.weight',
 'module.model.visual.transformer.resblocks.9.ln_1.bias',
 'module.model.visual.transformer.resblocks.9.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.9.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.9.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.9.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.9.ln_2.weight',
 'module.model.visual.transformer.resblocks.9.ln_2.bias',
 'module.model.visual.transformer.resblocks.10.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.10.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.10.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.10.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.10.ln_1.weight',
 'module.model.visual.transformer.resblocks.10.ln_1.bias',
 'module.model.visual.transformer.resblocks.10.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.10.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.10.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.10.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.10.ln_2.weight',
 'module.model.visual.transformer.resblocks.10.ln_2.bias',
 'module.model.visual.transformer.resblocks.11.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.11.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.11.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.11.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.11.ln_1.weight',
 'module.model.visual.transformer.resblocks.11.ln_1.bias',
 'module.model.visual.transformer.resblocks.11.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.11.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.11.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.11.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.11.ln_2.weight',
 'module.model.visual.transformer.resblocks.11.ln_2.bias',
 'module.model.visual.ln_post.weight',
 'module.model.visual.ln_post.bias',
 'module.model.transformer.resblocks.0.attn.in_proj_weight',
 'module.model.transformer.resblocks.0.attn.in_proj_bias',
 'module.model.transformer.resblocks.0.attn.out_proj.weight',
 'module.model.transformer.resblocks.0.attn.out_proj.bias',
 'module.model.transformer.resblocks.0.ln_1.weight',
 'module.model.transformer.resblocks.0.ln_1.bias',
 'module.model.transformer.resblocks.0.mlp.c_fc.weight',
 'module.model.transformer.resblocks.0.mlp.c_fc.bias',
 'module.model.transformer.resblocks.0.mlp.c_proj.weight',
 'module.model.transformer.resblocks.0.mlp.c_proj.bias',
 'module.model.transformer.resblocks.0.ln_2.weight',
 'module.model.transformer.resblocks.0.ln_2.bias',
 'module.model.transformer.resblocks.1.attn.in_proj_weight',
 'module.model.transformer.resblocks.1.attn.in_proj_bias',
 'module.model.transformer.resblocks.1.attn.out_proj.weight',
 'module.model.transformer.resblocks.1.attn.out_proj.bias',
 'module.model.transformer.resblocks.1.ln_1.weight',
 'module.model.transformer.resblocks.1.ln_1.bias',
 'module.model.transformer.resblocks.1.mlp.c_fc.weight',
 'module.model.transformer.resblocks.1.mlp.c_fc.bias',
 'module.model.transformer.resblocks.1.mlp.c_proj.weight',
 'module.model.transformer.resblocks.1.mlp.c_proj.bias',
 'module.model.transformer.resblocks.1.ln_2.weight',
 'module.model.transformer.resblocks.1.ln_2.bias',
 'module.model.transformer.resblocks.2.attn.in_proj_weight',
 'module.model.transformer.resblocks.2.attn.in_proj_bias',
 'module.model.transformer.resblocks.2.attn.out_proj.weight',
 'module.model.transformer.resblocks.2.attn.out_proj.bias',
 'module.model.transformer.resblocks.2.ln_1.weight',
 'module.model.transformer.resblocks.2.ln_1.bias',
 'module.model.transformer.resblocks.2.mlp.c_fc.weight',
 'module.model.transformer.resblocks.2.mlp.c_fc.bias',
 'module.model.transformer.resblocks.2.mlp.c_proj.weight',
 'module.model.transformer.resblocks.2.mlp.c_proj.bias',
 'module.model.transformer.resblocks.2.ln_2.weight',
 'module.model.transformer.resblocks.2.ln_2.bias',
 'module.model.transformer.resblocks.3.attn.in_proj_weight',
 'module.model.transformer.resblocks.3.attn.in_proj_bias',
 'module.model.transformer.resblocks.3.attn.out_proj.weight',
 'module.model.transformer.resblocks.3.attn.out_proj.bias',
 'module.model.transformer.resblocks.3.ln_1.weight',
 'module.model.transformer.resblocks.3.ln_1.bias',
 'module.model.transformer.resblocks.3.mlp.c_fc.weight',
 'module.model.transformer.resblocks.3.mlp.c_fc.bias',
 'module.model.transformer.resblocks.3.mlp.c_proj.weight',
 'module.model.transformer.resblocks.3.mlp.c_proj.bias',
 'module.model.transformer.resblocks.3.ln_2.weight',
 'module.model.transformer.resblocks.3.ln_2.bias',
 'module.model.transformer.resblocks.4.attn.in_proj_weight',
 'module.model.transformer.resblocks.4.attn.in_proj_bias',
 'module.model.transformer.resblocks.4.attn.out_proj.weight',
 'module.model.transformer.resblocks.4.attn.out_proj.bias',
 'module.model.transformer.resblocks.4.ln_1.weight',
 'module.model.transformer.resblocks.4.ln_1.bias',
 'module.model.transformer.resblocks.4.mlp.c_fc.weight',
 'module.model.transformer.resblocks.4.mlp.c_fc.bias',
 'module.model.transformer.resblocks.4.mlp.c_proj.weight',
 'module.model.transformer.resblocks.4.mlp.c_proj.bias',
 'module.model.transformer.resblocks.4.ln_2.weight',
 'module.model.transformer.resblocks.4.ln_2.bias',
 'module.model.transformer.resblocks.5.attn.in_proj_weight',
 'module.model.transformer.resblocks.5.attn.in_proj_bias',
 'module.model.transformer.resblocks.5.attn.out_proj.weight',
 'module.model.transformer.resblocks.5.attn.out_proj.bias',
 'module.model.transformer.resblocks.5.ln_1.weight',
 'module.model.transformer.resblocks.5.ln_1.bias',
 'module.model.transformer.resblocks.5.mlp.c_fc.weight',
 'module.model.transformer.resblocks.5.mlp.c_fc.bias',
 'module.model.transformer.resblocks.5.mlp.c_proj.weight',
 'module.model.transformer.resblocks.5.mlp.c_proj.bias',
 'module.model.transformer.resblocks.5.ln_2.weight',
 'module.model.transformer.resblocks.5.ln_2.bias',
 'module.model.transformer.resblocks.6.attn.in_proj_weight',
 'module.model.transformer.resblocks.6.attn.in_proj_bias',
 'module.model.transformer.resblocks.6.attn.out_proj.weight',
 'module.model.transformer.resblocks.6.attn.out_proj.bias',
 'module.model.transformer.resblocks.6.ln_1.weight',
 'module.model.transformer.resblocks.6.ln_1.bias',
 'module.model.transformer.resblocks.6.mlp.c_fc.weight',
 'module.model.transformer.resblocks.6.mlp.c_fc.bias',
 'module.model.transformer.resblocks.6.mlp.c_proj.weight',
 'module.model.transformer.resblocks.6.mlp.c_proj.bias',
 'module.model.transformer.resblocks.6.ln_2.weight',
 'module.model.transformer.resblocks.6.ln_2.bias',
 'module.model.transformer.resblocks.7.attn.in_proj_weight',
 'module.model.transformer.resblocks.7.attn.in_proj_bias',
 'module.model.transformer.resblocks.7.attn.out_proj.weight',
 'module.model.transformer.resblocks.7.attn.out_proj.bias',
 'module.model.transformer.resblocks.7.ln_1.weight',
 'module.model.transformer.resblocks.7.ln_1.bias',
 'module.model.transformer.resblocks.7.mlp.c_fc.weight',
 'module.model.transformer.resblocks.7.mlp.c_fc.bias',
 'module.model.transformer.resblocks.7.mlp.c_proj.weight',
 'module.model.transformer.resblocks.7.mlp.c_proj.bias',
 'module.model.transformer.resblocks.7.ln_2.weight',
 'module.model.transformer.resblocks.7.ln_2.bias',
 'module.model.transformer.resblocks.8.attn.in_proj_weight',
 'module.model.transformer.resblocks.8.attn.in_proj_bias',
 'module.model.transformer.resblocks.8.attn.out_proj.weight',
 'module.model.transformer.resblocks.8.attn.out_proj.bias',
 'module.model.transformer.resblocks.8.ln_1.weight',
 'module.model.transformer.resblocks.8.ln_1.bias',
 'module.model.transformer.resblocks.8.mlp.c_fc.weight',
 'module.model.transformer.resblocks.8.mlp.c_fc.bias',
 'module.model.transformer.resblocks.8.mlp.c_proj.weight',
 'module.model.transformer.resblocks.8.mlp.c_proj.bias',
 'module.model.transformer.resblocks.8.ln_2.weight',
 'module.model.transformer.resblocks.8.ln_2.bias',
 'module.model.transformer.resblocks.9.attn.in_proj_weight',
 'module.model.transformer.resblocks.9.attn.in_proj_bias',
 'module.model.transformer.resblocks.9.attn.out_proj.weight',
 'module.model.transformer.resblocks.9.attn.out_proj.bias',
 'module.model.transformer.resblocks.9.ln_1.weight',
 'module.model.transformer.resblocks.9.ln_1.bias',
 'module.model.transformer.resblocks.9.mlp.c_fc.weight',
 'module.model.transformer.resblocks.9.mlp.c_fc.bias',
 'module.model.transformer.resblocks.9.mlp.c_proj.weight',
 'module.model.transformer.resblocks.9.mlp.c_proj.bias',
 'module.model.transformer.resblocks.9.ln_2.weight',
 'module.model.transformer.resblocks.9.ln_2.bias',
 'module.model.transformer.resblocks.10.attn.in_proj_weight',
 'module.model.transformer.resblocks.10.attn.in_proj_bias',
 'module.model.transformer.resblocks.10.attn.out_proj.weight',
 'module.model.transformer.resblocks.10.attn.out_proj.bias',
 'module.model.transformer.resblocks.10.ln_1.weight',
 'module.model.transformer.resblocks.10.ln_1.bias',
 'module.model.transformer.resblocks.10.mlp.c_fc.weight',
 'module.model.transformer.resblocks.10.mlp.c_fc.bias',
 'module.model.transformer.resblocks.10.mlp.c_proj.weight',
 'module.model.transformer.resblocks.10.mlp.c_proj.bias',
 'module.model.transformer.resblocks.10.ln_2.weight',
 'module.model.transformer.resblocks.10.ln_2.bias',
 'module.model.transformer.resblocks.11.attn.in_proj_weight',
 'module.model.transformer.resblocks.11.attn.in_proj_bias',
 'module.model.transformer.resblocks.11.attn.out_proj.weight',
 'module.model.transformer.resblocks.11.attn.out_proj.bias',
 'module.model.transformer.resblocks.11.ln_1.weight',
 'module.model.transformer.resblocks.11.ln_1.bias',
 'module.model.transformer.resblocks.11.mlp.c_fc.weight',
 'module.model.transformer.resblocks.11.mlp.c_fc.bias',
 'module.model.transformer.resblocks.11.mlp.c_proj.weight',
 'module.model.transformer.resblocks.11.mlp.c_proj.bias',
 'module.model.transformer.resblocks.11.ln_2.weight',
 'module.model.transformer.resblocks.11.ln_2.bias',
 'module.model.token_embedding.weight',
 'module.model.ln_final.weight',
 'module.model.ln_final.bias',
 'module.projector_v.0.weight',
 'module.projector_v.2.weight',
 'module.projector_t.0.weight',
 'module.projector_t.2.weight']
[12/16 23:53:53][INFO] misc.py:  185: Model:
DistributedDataParallel(
  (module): TemporalClipVideo(
    (model): WCLIP(
      (visual): TemporalVisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TSTransformer(
          (resblocks): Sequential(
            (0): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (raw_model): WCLIP(
      (visual): TemporalVisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TSTransformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (projector_v): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): GELU()
      (2): Linear(in_features=512, out_features=512, bias=False)
    )
    (projector_t): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): GELU()
      (2): Linear(in_features=512, out_features=512, bias=False)
    )
  )
)
[12/16 23:53:53][INFO] misc.py:  187: Params: 300,290,050
[12/16 23:53:53][INFO] misc.py:  188: Mem: 1.6981406211853027 MB
[12/16 23:53:53][INFO] misc.py:  197: nvidia-smi
Mon Dec 16 23:53:53 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  Off |
| 42%   24C    P2    66W / 450W |   5130MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:23:00.0 Off |                  Off |
| 41%   25C    P2    63W / 450W |   5130MiB / 24564MiB |     34%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA GeForce ...  Off  | 00000000:41:00.0 Off |                  Off |
| 41%   26C    P2    68W / 450W |   5130MiB / 24564MiB |      6%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA GeForce ...  Off  | 00000000:61:00.0 Off |                  Off |
| 40%   25C    P2    55W / 450W |   5130MiB / 24564MiB |     28%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA GeForce ...  Off  | 00000000:81:00.0 Off |                  Off |
| 42%   25C    P2    70W / 450W |   5130MiB / 24564MiB |     27%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA GeForce ...  Off  | 00000000:A1:00.0 Off |                  Off |
| 41%   25C    P2    69W / 450W |   5130MiB / 24564MiB |     52%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA GeForce ...  Off  | 00000000:C1:00.0 Off |                  Off |
| 42%   25C    P2    59W / 450W |   5130MiB / 24564MiB |     49%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA GeForce ...  Off  | 00000000:E1:00.0 Off |                  Off |
| 40%   25C    P2    67W / 450W |   5130MiB / 24564MiB |     14%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      3082      G   /usr/lib/xorg/Xorg                  4MiB |
|    0   N/A  N/A   3749450      C   .../envs/slowfast/bin/python     5122MiB |
|    1   N/A  N/A      3082      G   /usr/lib/xorg/Xorg                  4MiB |
|    1   N/A  N/A   3749451      C   .../envs/slowfast/bin/python     5122MiB |
|    2   N/A  N/A      3082      G   /usr/lib/xorg/Xorg                  4MiB |
|    2   N/A  N/A   3749452      C   .../envs/slowfast/bin/python     5122MiB |
|    3   N/A  N/A      3082      G   /usr/lib/xorg/Xorg                  4MiB |
|    3   N/A  N/A   3749453      C   .../envs/slowfast/bin/python     5122MiB |
|    4   N/A  N/A      3082      G   /usr/lib/xorg/Xorg                  4MiB |
|    4   N/A  N/A   3749454      C   .../envs/slowfast/bin/python     5122MiB |
|    5   N/A  N/A      3082      G   /usr/lib/xorg/Xorg                  4MiB |
|    5   N/A  N/A   3749460      C   .../envs/slowfast/bin/python     5122MiB |
|    6   N/A  N/A      3082      G   /usr/lib/xorg/Xorg                  4MiB |
|    6   N/A  N/A   3749461      C   .../envs/slowfast/bin/python     5122MiB |
|    7   N/A  N/A      3082      G   /usr/lib/xorg/Xorg                  4MiB |
|    7   N/A  N/A   3749462      C   .../envs/slowfast/bin/python     5122MiB |
+-----------------------------------------------------------------------------+
bn 0, non bn 107, zero 199, no grad 302
[12/16 23:53:54][INFO] kinetics.py:   94: Constructing Kinetics train...
path: ---------------------------- /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_ssv2/train.csv
[12/16 23:53:54][INFO] kinetics.py:  172: Constructing kinetics dataloader (size: 1392 skip_rows 0) from /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_ssv2/train.csv 
[12/16 23:53:54][INFO] kinetics.py:   94: Constructing Kinetics val...
path: ---------------------------- /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_ssv2/test.csv
[12/16 23:53:54][INFO] kinetics.py:  172: Constructing kinetics dataloader (size: 7215 skip_rows 0) from /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_ssv2/test.csv 
[12/16 23:53:54][INFO] kinetics.py:   94: Constructing Kinetics train...
path: ---------------------------- /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_ssv2/train.csv
[12/16 23:53:54][INFO] kinetics.py:  172: Constructing kinetics dataloader (size: 1392 skip_rows 0) from /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_ssv2/train.csv 
[12/16 23:53:54][INFO] train_net.py: 1134: Start epoch: 1
[12/16 23:53:54][INFO] train_net.py:  304: total trainable params:
[12/16 23:53:54][INFO] train_net.py:  307: module.model.positional_embedding
[12/16 23:53:54][INFO] train_net.py:  307: module.model.text_projection
[12/16 23:53:54][INFO] train_net.py:  307: module.model.logit_scale
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.class_embedding
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.positional_embedding
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.proj
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.conv1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.ln_pre.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.ln_pre.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.ln_post.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.visual.ln_post.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.0.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.1.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.2.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.3.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.4.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.5.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.6.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.7.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.8.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.9.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.10.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.attn.in_proj_weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.attn.in_proj_bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.attn.out_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.attn.out_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.ln_1.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.ln_1.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.mlp.c_fc.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.mlp.c_fc.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.mlp.c_proj.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.mlp.c_proj.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.ln_2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.transformer.resblocks.11.ln_2.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.model.token_embedding.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.ln_final.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.model.ln_final.bias
[12/16 23:53:54][INFO] train_net.py:  307: module.projector_v.0.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.projector_v.2.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.projector_t.0.weight
[12/16 23:53:54][INFO] train_net.py:  307: module.projector_t.2.weight
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
exponential temporal pooling in train
alpha:0.2
[12/16 23:54:14][INFO] train_net.py:  491: Distillation Loss: 0.07358217
[12/16 23:54:14][INFO] train_net.py:  492: Distillation Loss Ratio: 2.000000
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
[12/16 23:54:21][INFO] logging.py:   99: json_stats: {"_type": "train_iter_", "dt": 0.65791841, "dt_data": 0.00040911, "dt_net": 0.65750777, "epoch": "1/12", "eta": "0:05:32", "gpu_mem": "13.72G", "grad_norm": 115.61530304, "iter": "10/43", "loss": 5.35076165, "lr": 3.8E-7, "top1_err": 98.43750000, "top5_err": 87.50000000}
exponential temporal pooling in train
alpha:0.2
[12/16 23:54:21][INFO] train_net.py:  491: Distillation Loss: 0.07116610
[12/16 23:54:21][INFO] train_net.py:  492: Distillation Loss Ratio: 2.000000
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
[12/16 23:54:27][INFO] logging.py:   99: json_stats: {"_type": "train_iter_", "dt": 0.63925405, "dt_data": 0.00037083, "dt_net": 0.63888162, "epoch": "1/12", "eta": "0:05:17", "gpu_mem": "13.72G", "grad_norm": 102.47490692, "iter": "20/43", "loss": 4.87714505, "lr": 7.6E-7, "top1_err": 96.87500000, "top5_err": 84.37500000}
exponential temporal pooling in train
alpha:0.2
[12/16 23:54:27][INFO] train_net.py:  491: Distillation Loss: 0.06069726
[12/16 23:54:27][INFO] train_net.py:  492: Distillation Loss Ratio: 2.000000
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
[12/16 23:54:33][INFO] logging.py:   99: json_stats: {"_type": "train_iter_", "dt": 0.64082236, "dt_data": 0.00019110, "dt_net": 0.64063041, "epoch": "1/12", "eta": "0:05:11", "gpu_mem": "13.72G", "grad_norm": 68.24732208, "iter": "30/43", "loss": 4.55520511, "lr": 0.00000114, "top1_err": 96.87500000, "top5_err": 81.25000000}
exponential temporal pooling in train
alpha:0.2
[12/16 23:54:34][INFO] train_net.py:  491: Distillation Loss: 0.08563775
[12/16 23:54:34][INFO] train_net.py:  492: Distillation Loss Ratio: 2.000000
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
[12/16 23:54:40][INFO] logging.py:   99: json_stats: {"_type": "train_iter_", "dt": 0.65442897, "dt_data": 0.00017461, "dt_net": 0.65425342, "epoch": "1/12", "eta": "0:05:11", "gpu_mem": "13.72G", "grad_norm": 70.29114532, "iter": "40/43", "loss": 4.52968144, "lr": 0.00000153, "top1_err": 93.75000000, "top5_err": 84.37500000}
exponential temporal pooling in train
alpha:0.2
[12/16 23:54:40][INFO] train_net.py:  491: Distillation Loss: 0.07880193
[12/16 23:54:40][INFO] train_net.py:  492: Distillation Loss Ratio: 2.000000
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
[12/16 23:54:44][INFO] logging.py:   99: json_stats: {"RAM": "49.58/251.74G", "_type": "train_epoch", "dt": 2.38412264, "dt_data": 2.38412204, "dt_net": 0.63799767, "epoch": "1/12", "eta": "0:18:47", "gpu_mem": "13.72G", "grad_norm": 57.19869995, "loss": 4.77348292, "lr": 0.00000164, "top1_err": 95.56686047, "top5_err": 84.01162791}
[12/16 23:54:44][INFO] train_net.py: 1338: Epoch 0 takes 50.44s. Epochs from 0 to 0 take 50.44s in average and 50.44s in median.
[12/16 23:54:44][INFO] train_net.py: 1344: For epoch 0, each iteraction takes 1.17s in average. From epoch 0 to 0, each iteraction takes 1.17s in average.
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:55:17][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:03:25", "gpu_mem": "13.72G", "iter": "10/226", "time_diff": 0.95151930, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:55:26][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:03:15", "gpu_mem": "13.72G", "iter": "20/226", "time_diff": 0.95111232, "top1_err": 100.00000000, "top5_err": 95.31250000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:55:36][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:03:04", "gpu_mem": "13.72G", "iter": "30/226", "time_diff": 0.94212095, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:55:46][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:02:54", "gpu_mem": "13.72G", "iter": "40/226", "time_diff": 0.93856924, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:55:55][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:02:45", "gpu_mem": "13.72G", "iter": "50/226", "time_diff": 0.93872262, "top1_err": 100.00000000, "top5_err": 90.62500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:56:05][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:02:35", "gpu_mem": "13.72G", "iter": "60/226", "time_diff": 0.93630367, "top1_err": 100.00000000, "top5_err": 90.62500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:56:14][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:02:26", "gpu_mem": "13.72G", "iter": "70/226", "time_diff": 0.93778980, "top1_err": 100.00000000, "top5_err": 96.87500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:56:24][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:02:19", "gpu_mem": "13.72G", "iter": "80/226", "time_diff": 0.95247914, "top1_err": 100.00000000, "top5_err": 96.87500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:56:33][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:02:11", "gpu_mem": "13.72G", "iter": "90/226", "time_diff": 0.96839411, "top1_err": 100.00000000, "top5_err": 95.31250000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:56:43][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:02:00", "gpu_mem": "13.72G", "iter": "100/226", "time_diff": 0.95823109, "top1_err": 96.87500000, "top5_err": 95.31250000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:56:52][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:01:53", "gpu_mem": "13.72G", "iter": "110/226", "time_diff": 0.97461816, "top1_err": 100.00000000, "top5_err": 95.31250000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:57:02][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:01:40", "gpu_mem": "13.72G", "iter": "120/226", "time_diff": 0.94569145, "top1_err": 100.00000000, "top5_err": 95.31250000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:57:11][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:01:30", "gpu_mem": "13.72G", "iter": "130/226", "time_diff": 0.93952959, "top1_err": 100.00000000, "top5_err": 96.87500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:57:21][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:01:22", "gpu_mem": "13.72G", "iter": "140/226", "time_diff": 0.95478857, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:57:30][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:01:11", "gpu_mem": "13.72G", "iter": "150/226", "time_diff": 0.94713276, "top1_err": 100.00000000, "top5_err": 92.18750000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:57:40][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:01:01", "gpu_mem": "13.72G", "iter": "160/226", "time_diff": 0.93194482, "top1_err": 100.00000000, "top5_err": 95.31250000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:57:49][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:00:53", "gpu_mem": "13.72G", "iter": "170/226", "time_diff": 0.95115636, "top1_err": 98.43750000, "top5_err": 95.31250000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:57:59][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:00:43", "gpu_mem": "13.72G", "iter": "180/226", "time_diff": 0.94337803, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:58:08][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:00:33", "gpu_mem": "13.72G", "iter": "190/226", "time_diff": 0.94054618, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:58:18][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:00:24", "gpu_mem": "13.72G", "iter": "200/226", "time_diff": 0.96040540, "top1_err": 100.00000000, "top5_err": 96.87500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:58:27][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:00:14", "gpu_mem": "13.72G", "iter": "210/226", "time_diff": 0.91812723, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:58:36][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "1/12", "eta": "0:00:05", "gpu_mem": "13.72G", "iter": "220/226", "time_diff": 0.90858336, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/16 23:58:44][INFO] logging.py:   99: json_stats: {"RAM": "50.46/251.74G", "_type": "val_epoch", "epoch": "1/12", "gpu_mem": "13.72G", "min_top1_err": 98.98835920, "min_top5_err": 93.76385809, "time_diff": 2.08725634, "top1_err": 98.98835920, "top5_err": 93.76385809}
[12/16 23:58:44][INFO] train_net.py:  304: total trainable params:
[12/16 23:58:44][INFO] train_net.py:  307: module.model.positional_embedding
[12/16 23:58:44][INFO] train_net.py:  307: module.model.text_projection
[12/16 23:58:44][INFO] train_net.py:  307: module.model.logit_scale
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.class_embedding
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.positional_embedding
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.proj
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.conv1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.ln_pre.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.ln_pre.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.0.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.1.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.2.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.3.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.4.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.5.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.6.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.7.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.8.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.9.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.10.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.transformer.resblocks.11.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.ln_post.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.visual.ln_post.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.0.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.1.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.2.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.3.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.4.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.5.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.6.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.7.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.8.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.9.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.10.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.attn.in_proj_weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.attn.in_proj_bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.attn.out_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.attn.out_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.ln_1.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.ln_1.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.mlp.c_fc.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.mlp.c_fc.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.mlp.c_proj.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.mlp.c_proj.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.ln_2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.transformer.resblocks.11.ln_2.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.model.token_embedding.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.ln_final.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.model.ln_final.bias
[12/16 23:58:44][INFO] train_net.py:  307: module.projector_v.0.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.projector_v.2.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.projector_t.0.weight
[12/16 23:58:44][INFO] train_net.py:  307: module.projector_t.2.weight
exponential temporal pooling in train
alpha:0.2
[12/16 23:59:03][INFO] train_net.py:  491: Distillation Loss: 0.08385408
[12/16 23:59:03][INFO] train_net.py:  492: Distillation Loss Ratio: 2.000000
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
[12/16 23:59:11][INFO] logging.py:   99: json_stats: {"_type": "train_iter_", "dt": 0.65230748, "dt_data": 0.00049562, "dt_net": 0.65180973, "epoch": "2/12", "eta": "0:05:02", "gpu_mem": "13.77G", "grad_norm": 64.34636688, "iter": "10/43", "loss": 4.15592575, "lr": 0.00000203, "top1_err": 92.18750000, "top5_err": 75.00000000}
exponential temporal pooling in train
alpha:0.2
[12/16 23:59:11][INFO] train_net.py:  491: Distillation Loss: 0.07545131
[12/16 23:59:11][INFO] train_net.py:  492: Distillation Loss Ratio: 2.000000
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
[12/16 23:59:17][INFO] logging.py:   99: json_stats: {"_type": "train_iter_", "dt": 0.64419753, "dt_data": 0.00044572, "dt_net": 0.64375047, "epoch": "2/12", "eta": "0:04:51", "gpu_mem": "13.77G", "grad_norm": 67.68451691, "iter": "20/43", "loss": 4.13085461, "lr": 0.00000241, "top1_err": 92.18750000, "top5_err": 75.00000000}
exponential temporal pooling in train
alpha:0.2
[12/16 23:59:18][INFO] train_net.py:  491: Distillation Loss: 0.08840418
[12/16 23:59:18][INFO] train_net.py:  492: Distillation Loss Ratio: 2.000000
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
[12/16 23:59:24][INFO] logging.py:   99: json_stats: {"_type": "train_iter_", "dt": 0.64143282, "dt_data": 0.00018413, "dt_net": 0.64124754, "epoch": "2/12", "eta": "0:04:44", "gpu_mem": "13.77G", "grad_norm": 72.33232880, "iter": "30/43", "loss": 4.06984115, "lr": 0.00000279, "top1_err": 87.50000000, "top5_err": 73.43750000}
exponential temporal pooling in train
alpha:0.2
[12/16 23:59:24][INFO] train_net.py:  491: Distillation Loss: 0.07908440
[12/16 23:59:24][INFO] train_net.py:  492: Distillation Loss Ratio: 2.000000
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
[12/16 23:59:30][INFO] logging.py:   99: json_stats: {"_type": "train_iter_", "dt": 0.64003561, "dt_data": 0.00018651, "dt_net": 0.63984889, "epoch": "2/12", "eta": "0:04:37", "gpu_mem": "13.77G", "grad_norm": 67.20252228, "iter": "40/43", "loss": 4.08873272, "lr": 0.00000318, "top1_err": 87.50000000, "top5_err": 68.75000000}
exponential temporal pooling in train
alpha:0.2
[12/16 23:59:30][INFO] train_net.py:  491: Distillation Loss: 0.09440273
[12/16 23:59:30][INFO] train_net.py:  492: Distillation Loss Ratio: 2.000000
exponential temporal pooling in train
alpha:0.2
exponential temporal pooling in train
alpha:0.2
[12/16 23:59:35][INFO] logging.py:   99: json_stats: {"RAM": "49.61/251.74G", "_type": "train_epoch", "dt": 2.36155980, "dt_data": 2.36155869, "dt_net": 0.63393616, "epoch": "2/12", "eta": "0:16:55", "gpu_mem": "13.77G", "grad_norm": 70.32186127, "loss": 4.10768702, "lr": 0.00000329, "top1_err": 89.46220930, "top5_err": 72.38372093}
[12/16 23:59:35][INFO] train_net.py: 1338: Epoch 1 takes 50.72s. Epochs from 0 to 1 take 50.58s in average and 50.58s in median.
[12/16 23:59:35][INFO] train_net.py: 1344: For epoch 1, each iteraction takes 1.18s in average. From epoch 0 to 1, each iteraction takes 1.18s in average.
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:00:07][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:03:25", "gpu_mem": "13.77G", "iter": "10/226", "time_diff": 0.94941334, "top1_err": 100.00000000, "top5_err": 96.87500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:00:17][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:03:12", "gpu_mem": "13.77G", "iter": "20/226", "time_diff": 0.93381066, "top1_err": 100.00000000, "top5_err": 92.18750000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:00:26][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:03:09", "gpu_mem": "13.77G", "iter": "30/226", "time_diff": 0.96598116, "top1_err": 100.00000000, "top5_err": 92.18750000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:00:36][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:02:55", "gpu_mem": "13.77G", "iter": "40/226", "time_diff": 0.94147697, "top1_err": 100.00000000, "top5_err": 96.87500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:00:45][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:02:46", "gpu_mem": "13.77G", "iter": "50/226", "time_diff": 0.94379951, "top1_err": 96.87500000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:00:55][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:02:38", "gpu_mem": "13.77G", "iter": "60/226", "time_diff": 0.95492108, "top1_err": 100.00000000, "top5_err": 92.18750000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:01:04][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:02:28", "gpu_mem": "13.77G", "iter": "70/226", "time_diff": 0.95504745, "top1_err": 100.00000000, "top5_err": 96.87500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:01:14][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:02:18", "gpu_mem": "13.77G", "iter": "80/226", "time_diff": 0.94591009, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:01:23][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:02:09", "gpu_mem": "13.77G", "iter": "90/226", "time_diff": 0.94920035, "top1_err": 96.87500000, "top5_err": 92.18750000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:01:32][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:01:57", "gpu_mem": "13.77G", "iter": "100/226", "time_diff": 0.93481936, "top1_err": 100.00000000, "top5_err": 95.31250000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:01:42][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:01:50", "gpu_mem": "13.77G", "iter": "110/226", "time_diff": 0.95297994, "top1_err": 100.00000000, "top5_err": 96.87500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:01:51][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:01:40", "gpu_mem": "13.77G", "iter": "120/226", "time_diff": 0.94463260, "top1_err": 100.00000000, "top5_err": 95.31250000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:02:01][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:01:31", "gpu_mem": "13.77G", "iter": "130/226", "time_diff": 0.95619567, "top1_err": 100.00000000, "top5_err": 96.87500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:02:10][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:01:20", "gpu_mem": "13.77G", "iter": "140/226", "time_diff": 0.93961904, "top1_err": 100.00000000, "top5_err": 90.62500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:02:20][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:01:13", "gpu_mem": "13.77G", "iter": "150/226", "time_diff": 0.96966272, "top1_err": 98.43750000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:02:29][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:01:03", "gpu_mem": "13.77G", "iter": "160/226", "time_diff": 0.96260762, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:02:39][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:00:52", "gpu_mem": "13.77G", "iter": "170/226", "time_diff": 0.93731169, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:02:48][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:00:42", "gpu_mem": "13.77G", "iter": "180/226", "time_diff": 0.93090009, "top1_err": 100.00000000, "top5_err": 96.87500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:02:58][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:00:34", "gpu_mem": "13.77G", "iter": "190/226", "time_diff": 0.96250382, "top1_err": 100.00000000, "top5_err": 93.75000000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:03:07][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:00:24", "gpu_mem": "13.77G", "iter": "200/226", "time_diff": 0.95640768, "top1_err": 100.00000000, "top5_err": 96.87500000}
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
exponential temporal pooling in test
alpha:0.2
[12/17 00:03:17][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "2/12", "eta": "0:00:14", "gpu_mem": "13.77G", "iter": "210/226", "time_diff": 0.93210740, "top1_err": 100.00000000, "top5_err": 93.75000000}
