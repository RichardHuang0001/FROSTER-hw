nohup: ignoring input
config files: ['configs/Kinetics/TemporalCLIP_vitb16_8x16_STAdapter_HMDB51.yaml']
[11/29 23:18:21][INFO] train_net.py:  937: --Train with config:
[11/29 23:18:21][INFO] train_net.py:  938: {'AUG': {'AA_TYPE': 'rand-m7-n4-mstd0.5-inc1',
         'COLOR_JITTER': 0.4,
         'ENABLE': False,
         'GEN_MASK_LOADER': False,
         'INTERPOLATION': 'bicubic',
         'MASK_FRAMES': False,
         'MASK_RATIO': 0.0,
         'MASK_TUBE': False,
         'MASK_WINDOW_SIZE': [8, 7, 7],
         'MAX_MASK_PATCHES_PER_BLOCK': None,
         'NUM_SAMPLE': 1,
         'RE_COUNT': 1,
         'RE_MODE': 'pixel',
         'RE_PROB': 0.25,
         'RE_SPLIT': False},
 'AVA': {'ANNOTATION_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/',
         'BGR': False,
         'DETECTION_SCORE_THRESH': 0.9,
         'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv',
         'FRAME_DIR': '/mnt/fair-flash3-east/ava_trainval_frames.img/',
         'FRAME_LIST_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/',
         'FULL_TEST_ON_VAL': False,
         'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv',
         'IMG_PROC_BACKEND': 'cv2',
         'LABEL_MAP_FILE': 'ava_action_list_v2.2_for_activitynet_2019.pbtxt',
         'TEST_FORCE_FLIP': False,
         'TEST_LISTS': ['val.csv'],
         'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'],
         'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'],
         'TRAIN_LISTS': ['train.csv'],
         'TRAIN_PCA_JITTER_ONLY': True,
         'TRAIN_PREDICT_BOX_LISTS': [],
         'TRAIN_USE_COLOR_AUGMENTATION': False},
 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}),
 'BN': {'GLOBAL_SYNC': False,
        'NORM_TYPE': 'batchnorm',
        'NUM_BATCHES_PRECISE': 200,
        'NUM_SPLITS': 1,
        'NUM_SYNC_DEVICES': 1,
        'USE_PRECISE_STATS': False,
        'WEIGHT_DECAY': 0.0},
 'CONTRASTIVE': {'BN_MLP': False,
                 'BN_SYNC_MLP': False,
                 'DELTA_CLIPS_MAX': inf,
                 'DELTA_CLIPS_MIN': -inf,
                 'DIM': 128,
                 'INTERP_MEMORY': False,
                 'KNN_ON': True,
                 'LENGTH': 239975,
                 'LOCAL_SHUFFLE_BN': True,
                 'MEM_TYPE': '1d',
                 'MLP_DIM': 2048,
                 'MOCO_MULTI_VIEW_QUEUE': False,
                 'MOMENTUM': 0.5,
                 'MOMENTUM_ANNEALING': False,
                 'NUM_CLASSES_DOWNSTREAM': 400,
                 'NUM_MLP_LAYERS': 1,
                 'PREDICTOR_DEPTHS': [],
                 'QUEUE_LEN': 65536,
                 'SEQUENTIAL': False,
                 'SIMCLR_DIST_ON': True,
                 'SWAV_QEUE_LEN': 0,
                 'T': 0.07,
                 'TYPE': 'mem'},
 'DATA': {'COLOR_RND_GRAYSCALE': 0.0,
          'DECODING_BACKEND': 'pyav',
          'DECODING_SHORT_SIZE': 256,
          'DUMMY_LOAD': False,
          'ENSEMBLE_METHOD': 'sum',
          'IN22K_TRAINVAL': False,
          'IN22k_VAL_IN1K': '',
          'INDEX_LABEL_MAPPING_FILE': '/mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb/train_rephrased.json',
          'INPUT_CHANNEL_NUM': [3],
          'INV_UNIFORM_SAMPLE': False,
          'IN_VAL_CROP_RATIO': 0.875,
          'LOADER_CHUNK_OVERALL_SIZE': 0,
          'LOADER_CHUNK_SIZE': 0,
          'MEAN': [0.48145466, 0.4578275, 0.40821073],
          'MULTI_LABEL': False,
          'NUM_FRAMES': 8,
          'PATH_LABEL_SEPARATOR': ',',
          'PATH_PREFIX': '/mnt/SSD8T/home/huangwei/projects/FROSTER/data/hmdb51',
          'PATH_TO_DATA_DIR': '/mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb',
          'PATH_TO_PRELOAD_IMDB': '',
          'RANDOM_FLIP': True,
          'REVERSE_INPUT_CHANNEL': False,
          'SAMPLING_RATE': 16,
          'SKIP_ROWS': 0,
          'SSL_BLUR_SIGMA_MAX': [0.0, 2.0],
          'SSL_BLUR_SIGMA_MIN': [0.0, 0.1],
          'SSL_COLOR_BRI_CON_SAT': [0.4, 0.4, 0.4],
          'SSL_COLOR_HUE': 0.1,
          'SSL_COLOR_JITTER': False,
          'SSL_MOCOV2_AUG': False,
          'STD': [0.26862954, 0.26130258, 0.27577711],
          'TARGET_FPS': 30,
          'TEST_CROP_SIZE': 224,
          'TIME_DIFF_PROB': 0.0,
          'TRAIN_CROP_NUM_SPATIAL': 1,
          'TRAIN_CROP_NUM_TEMPORAL': 1,
          'TRAIN_CROP_SIZE': 224,
          'TRAIN_JITTER_ASPECT_RELATIVE': [],
          'TRAIN_JITTER_FPS': 0.0,
          'TRAIN_JITTER_MOTION_SHIFT': False,
          'TRAIN_JITTER_SCALES': [224, 256],
          'TRAIN_JITTER_SCALES_RELATIVE': [],
          'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229],
          'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009],
                               [-0.5808, -0.0045, -0.814],
                               [-0.5836, -0.6948, 0.4203]],
          'USE_OFFSET_SAMPLING': True},
 'DATA_LOADER': {'ENABLE_MULTI_THREAD_DECODE': False,
                 'NUM_WORKERS': 8,
                 'PIN_MEMORY': True},
 'DEMO': {'BUFFER_SIZE': 0,
          'CLIP_VIS_SIZE': 10,
          'COMMON_CLASS_NAMES': ['watch (a person)',
                                 'talk to (e.g., self, a person, a group)',
                                 'listen to (a person)',
                                 'touch (an object)',
                                 'carry/hold (an object)',
                                 'walk',
                                 'sit',
                                 'lie/sleep',
                                 'bend/bow (at the waist)'],
          'COMMON_CLASS_THRES': 0.7,
          'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml',
          'DETECTRON2_THRESH': 0.9,
          'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl',
          'DISPLAY_HEIGHT': 0,
          'DISPLAY_WIDTH': 0,
          'ENABLE': False,
          'FPS': 30,
          'GT_BOXES': '',
          'INPUT_FORMAT': 'BGR',
          'INPUT_VIDEO': '',
          'LABEL_FILE_PATH': '',
          'NUM_CLIPS_SKIP': 0,
          'NUM_VIS_INSTANCES': 2,
          'OUTPUT_FILE': '',
          'OUTPUT_FPS': -1,
          'PREDS_BOXES': '',
          'SLOWMO': 1,
          'STARTING_SECOND': 900,
          'THREAD_ENABLE': False,
          'UNCOMMON_CLASS_THRES': 0.3,
          'VIS_MODE': 'thres',
          'WEBCAM': -1},
 'DETECTION': {'ALIGNED': True,
               'ENABLE': False,
               'ROI_XFORM_RESOLUTION': 7,
               'SPATIAL_SCALE_FACTOR': 16},
 'DIST_BACKEND': 'nccl',
 'IMAGENET_SIMPLELABEL_PATH': None,
 'LOG_MODEL_INFO': True,
 'LOG_PERIOD': 10,
 'MASK': {'DECODER_DEPTH': 0,
          'DECODER_EMBED_DIM': 512,
          'DECODER_SEP_POS_EMBED': False,
          'DEC_KV_KERNEL': [],
          'DEC_KV_STRIDE': [],
          'ENABLE': False,
          'HEAD_TYPE': 'separate',
          'MAE_ON': False,
          'MAE_RND_MASK': False,
          'NORM_PRED_PIXEL': True,
          'PER_FRAME_MASKING': False,
          'PRED_HOG': False,
          'PRETRAIN_DEPTH': [15],
          'SCALE_INIT_BY_DEPTH': False,
          'TIME_STRIDE_LOSS': True},
 'MIXUP': {'ALPHA': 0.8,
           'CUTMIX_ALPHA': 1.0,
           'ENABLE': False,
           'LABEL_SMOOTH_VALUE': 0.1,
           'PROB': 1.0,
           'SWITCH_PROB': 0.5},
 'MODEL': {'ACT_CHECKPOINT': False,
           'ADAPT_FINETUNE_FACTOR': 1.0,
           'ARCH': 'vitb16',
           'CLS_LOSS_RATIO': 1.0,
           'CONTEXT_LENGTH': 77,
           'DEFAULT_FINETUNE_FACTOR': 1.0,
           'DETACH_FINAL_FC': False,
           'DISTILLATION_RATIO': 2.0,
           'DROPCONNECT_RATE': 0.0,
           'DROPOUT_RATE': 0.5,
           'ENSEMBLE_PRED': False,
           'ENSEMBLE_RAWMODEL_RATIO': 0.0,
           'EXPERT_FINETUNE_FACTOR': 1.0,
           'EXPERT_INSERT_LAYERS': [10, 11],
           'FC_INIT_STD': 0.01,
           'FINETUNE_FACTOR': 1.0,
           'FP16_ALLREDUCE': False,
           'FROZEN_BN': False,
           'HEAD_ACT': 'softmax',
           'KEEP_RAW_MODEL': True,
           'LOSS_FREQ_TYPE': 'mse',
           'LOSS_FUNC': 'soft_cross_entropy',
           'MLP_FINETUNE_FACTOR': 1.0,
           'MODEL_NAME': 'TemporalClipVideo',
           'MULTI_PATHWAY_ARCH': ['slowfast'],
           'NUM_CLASSES': 26,
           'NUM_EXPERTS': 0,
           'PROMPT_NUM': 1,
           'RAW_MODEL_DISTILLATION': True,
           'RECORD_ROUTING': False,
           'ROUTING_FINETUNE_FACTOR': 1.0,
           'ROUTING_FREQUENCE_CONSTRAIN': 0.5,
           'ROUTING_FREQ_CONS_FACTOR': 1.0,
           'ROUTING_TYPE': 'patch-level',
           'SINGLE_PATHWAY_ARCH': ['2d',
                                   'c2d',
                                   'i3d',
                                   'slow',
                                   'x3d',
                                   'mvit',
                                   'maskmvit',
                                   'vitb32',
                                   'vitb16',
                                   'vitl14'],
           'STATIC_GRAPH': False,
           'TEMPORAL_MODELING_TYPE': 'expand_temporal_view',
           'TEXT_PROMPT': False,
           'USE_CHECKPOINT': False},
 'MULTIGRID': {'BN_BASE_SIZE': 8,
               'DEFAULT_B': 0,
               'DEFAULT_S': 0,
               'DEFAULT_T': 0,
               'EPOCH_FACTOR': 1.5,
               'EVAL_FREQ': 3,
               'LONG_CYCLE': False,
               'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476),
                                      (0.5, 0.7071067811865476),
                                      (0.5, 1),
                                      (1, 1)],
               'LONG_CYCLE_SAMPLING_RATE': 0,
               'SHORT_CYCLE': False,
               'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476]},
 'MVIT': {'CLS_EMBED_ON': True,
          'DEPTH': 16,
          'DIM_MUL': [],
          'DIM_MUL_IN_ATT': False,
          'DROPOUT_RATE': 0.0,
          'DROPPATH_RATE': 0.1,
          'EMBED_DIM': 96,
          'HEAD_INIT_SCALE': 1.0,
          'HEAD_MUL': [],
          'LAYER_SCALE_INIT_VALUE': 0.0,
          'MLP_RATIO': 4.0,
          'MODE': 'conv',
          'NORM': 'layernorm',
          'NORM_STEM': False,
          'NUM_HEADS': 1,
          'PATCH_2D': False,
          'PATCH_KERNEL': [3, 7, 7],
          'PATCH_PADDING': [2, 4, 4],
          'PATCH_STRIDE': [2, 4, 4],
          'POOL_FIRST': False,
          'POOL_KVQ_KERNEL': None,
          'POOL_KV_STRIDE': [],
          'POOL_KV_STRIDE_ADAPTIVE': None,
          'POOL_Q_STRIDE': [],
          'QKV_BIAS': True,
          'REL_POS_SPATIAL': False,
          'REL_POS_TEMPORAL': False,
          'REL_POS_ZERO_INIT': False,
          'RESIDUAL_POOLING': False,
          'REV': {'BUFFER_LAYERS': [],
                  'ENABLE': False,
                  'PRE_Q_FUSION': 'avg',
                  'RESPATH_FUSE': 'concat',
                  'RES_PATH': 'conv'},
          'SEPARATE_QKV': False,
          'SEP_POS_EMBED': False,
          'USE_ABS_POS': True,
          'USE_FIXED_SINCOS_POS': False,
          'USE_MEAN_POOLING': False,
          'ZERO_DECAY_POS_CLS': True},
 'NONLOCAL': {'GROUP': [[1], [1], [1], [1]],
              'INSTANTIATION': 'dot_product',
              'LOCATION': [[[]], [[]], [[]], [[]]],
              'POOL': [[[1, 2, 2], [1, 2, 2]],
                       [[1, 2, 2], [1, 2, 2]],
                       [[1, 2, 2], [1, 2, 2]],
                       [[1, 2, 2], [1, 2, 2]]]},
 'NUM_GPUS': 8,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': '/mnt/SSD8T/home/huangwei/projects/FROSTER/checkpoints/basetraining/B2N_hmdb51_froster',
 'RESNET': {'DEPTH': 50,
            'INPLACE_RELU': True,
            'NUM_BLOCK_TEMP_KERNEL': [[3], [4], [6], [3]],
            'NUM_GROUPS': 1,
            'SPATIAL_DILATIONS': [[1], [1], [1], [1]],
            'SPATIAL_STRIDES': [[1], [2], [2], [2]],
            'STRIDE_1X1': False,
            'TRANS_FUNC': 'bottleneck_transform',
            'WIDTH_PER_GROUP': 64,
            'ZERO_INIT_FINAL_BN': False,
            'ZERO_INIT_FINAL_CONV': False},
 'RNG_SEED': 0,
 'SHARD_ID': 0,
 'SLOWFAST': {'ALPHA': 8,
              'BETA_INV': 8,
              'FUSION_CONV_CHANNEL_RATIO': 2,
              'FUSION_KERNEL_SZ': 5},
 'SOLVER': {'BASE_LR': 3.33e-06,
            'BASE_LR_SCALE_NUM_SHARDS': True,
            'BETAS': (0.9, 0.999),
            'CLIP_GRAD_L2NORM': 1.0,
            'CLIP_GRAD_VAL': None,
            'COSINE_AFTER_WARMUP': True,
            'COSINE_END_LR': 3.33e-08,
            'COSINE_RESTART_EPOCH': 0,
            'DAMPENING': 0.0,
            'GAMMA': 0.1,
            'LARS_ON': False,
            'LAYER_DECAY': 1.0,
            'LRS': [],
            'LR_POLICY': 'cosine',
            'MAX_EPOCH': 12,
            'MOMENTUM': 0.9,
            'NESTEROV': True,
            'OPTIMIZING_METHOD': 'adamw',
            'STEPS': [],
            'STEP_SIZE': 1,
            'WARMUP_EPOCHS': 2.0,
            'WARMUP_FACTOR': 0.1,
            'WARMUP_START_LR': 3.33e-08,
            'WEIGHT_DECAY': 0.01,
            'ZERO_WD_1D_PARAM': True},
 'TASK': '',
 'TENSORBOARD': {'CATEGORIES_PATH': '',
                 'CLASS_NAMES_PATH': '',
                 'CONFUSION_MATRIX': {'ENABLE': False,
                                      'FIGSIZE': [8, 8],
                                      'SUBSET_PATH': ''},
                 'ENABLE': False,
                 'HISTOGRAM': {'ENABLE': False,
                               'FIGSIZE': [8, 8],
                               'SUBSET_PATH': '',
                               'TOPK': 10},
                 'LOG_DIR': '',
                 'MODEL_VIS': {'ACTIVATIONS': False,
                               'COLORMAP': 'Pastel2',
                               'ENABLE': False,
                               'GRAD_CAM': {'COLORMAP': 'viridis',
                                            'ENABLE': True,
                                            'LAYER_LIST': [],
                                            'USE_TRUE_LABEL': False},
                               'INPUT_VIDEO': False,
                               'LAYER_LIST': [],
                               'MODEL_WEIGHTS': False,
                               'TOPK_PREDS': 1},
                 'PREDICTIONS_PATH': '',
                 'WRONG_PRED_VIS': {'ENABLE': False,
                                    'SUBSET_PATH': '',
                                    'TAG': 'Incorrectly classified videos.'}},
 'TEST': {'BATCH_SIZE': 240,
          'CHECKPOINT_FILE_PATH': '',
          'CHECKPOINT_TYPE': 'pytorch',
          'CLIP_ORI_PATH': None,
          'CUSTOM_LOAD': False,
          'CUSTOM_LOAD_FILE': None,
          'DATASET': 'kinetics',
          'ENABLE': True,
          'NUM_ENSEMBLE_VIEWS': 3,
          'NUM_SPATIAL_CROPS': 1,
          'NUM_TEMPORAL_CLIPS': [],
          'OPENSET': False,
          'PATCHING_MODEL': False,
          'PATCHING_RATIO': 0.5,
          'SAVE_RESULTS_PATH': '',
          'UPDATE_STATE': False},
 'TEST_FILE': 'test.csv',
 'TRAIN': {'ADAPT_ZS_CONS_RATIO': False,
           'AUTO_RESUME': True,
           'BATCH_SIZE': 32,
           'CHECKPOINT_CLEAR_NAME_PATTERN': (),
           'CHECKPOINT_EPOCH_RESET': False,
           'CHECKPOINT_FILE_PATH': '',
           'CHECKPOINT_INFLATE': False,
           'CHECKPOINT_IN_INIT': False,
           'CHECKPOINT_PERIOD': 1,
           'CHECKPOINT_TYPE': 'pytorch',
           'CLIP_ORI_PATH': '/mnt/SSD8T/home/huangwei/.cache/clip/ViT-B-16.pt',
           'CUSTOM_LOAD': False,
           'CUSTOM_LOAD_FILE': None,
           'DATASET': 'kinetics',
           'ENABLE': True,
           'EVAL_PERIOD': 1,
           'EWC_CONSTRAIN_RATIO': 1.0,
           'EWC_IDENTITY_FISHER': False,
           'EWC_IGNORE_LOGIT_SCALE': False,
           'EWC_LOAD_FILE': None,
           'EWC_SET': False,
           'KILL_LOSS_EXPLOSION_FACTOR': 0.0,
           'LINEAR_CONNECT_CLIMB': False,
           'LINEAR_CONNECT_LOSS_RATIO': 0.0,
           'LINEAR_CONNECT_SAMPLE': True,
           'LINEAR_CONNECT_SAMPLE_L': 0.4,
           'LINEAR_CONNECT_SAMPLE_R': 0.6,
           'MIXED_PRECISION': True,
           'ZS_CONS': False,
           'ZS_CONS_RATIO': 0.8,
           'ZS_INIT_CONS': False,
           'ZS_RESTART_CONS': False,
           'ZS_RESTART_EPOCH': -1},
 'TRAIN_FILE': 'train.csv',
 'TUNE_HEAD': False,
 'VAL_FILE': 'val.csv',
 'VAL_MODE': False,
 'VIS_MASK': CfgNode({'ENABLE': False}),
 'X3D': {'BN_LIN5': False,
         'BOTTLENECK_FACTOR': 1.0,
         'CHANNELWISE_3x3x3': True,
         'DEPTH_FACTOR': 1.0,
         'DIM_C1': 12,
         'DIM_C5': 2048,
         'SCALE_RES2': False,
         'WIDTH_FACTOR': 1.0}}
[11/29 23:18:27][INFO] temporalclip_video_model.py:  604: load pretrained CLIP:<All keys matched successfully>
[11/29 23:18:35][INFO] temporalclip_video_model.py:  604: load pretrained CLIP:<All keys matched successfully>
[11/29 23:18:37][INFO] train_net.py:  946: total trainable parameters:
[11/29 23:18:37][INFO] train_net.py:  947: ['module.model.positional_embedding',
 'module.model.text_projection',
 'module.model.logit_scale',
 'module.model.visual.class_embedding',
 'module.model.visual.positional_embedding',
 'module.model.visual.proj',
 'module.model.visual.conv1.weight',
 'module.model.visual.ln_pre.weight',
 'module.model.visual.ln_pre.bias',
 'module.model.visual.transformer.resblocks.0.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.0.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.0.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.0.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.0.ln_1.weight',
 'module.model.visual.transformer.resblocks.0.ln_1.bias',
 'module.model.visual.transformer.resblocks.0.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.0.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.0.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.0.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.0.ln_2.weight',
 'module.model.visual.transformer.resblocks.0.ln_2.bias',
 'module.model.visual.transformer.resblocks.1.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.1.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.1.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.1.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.1.ln_1.weight',
 'module.model.visual.transformer.resblocks.1.ln_1.bias',
 'module.model.visual.transformer.resblocks.1.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.1.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.1.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.1.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.1.ln_2.weight',
 'module.model.visual.transformer.resblocks.1.ln_2.bias',
 'module.model.visual.transformer.resblocks.2.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.2.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.2.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.2.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.2.ln_1.weight',
 'module.model.visual.transformer.resblocks.2.ln_1.bias',
 'module.model.visual.transformer.resblocks.2.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.2.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.2.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.2.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.2.ln_2.weight',
 'module.model.visual.transformer.resblocks.2.ln_2.bias',
 'module.model.visual.transformer.resblocks.3.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.3.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.3.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.3.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.3.ln_1.weight',
 'module.model.visual.transformer.resblocks.3.ln_1.bias',
 'module.model.visual.transformer.resblocks.3.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.3.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.3.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.3.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.3.ln_2.weight',
 'module.model.visual.transformer.resblocks.3.ln_2.bias',
 'module.model.visual.transformer.resblocks.4.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.4.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.4.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.4.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.4.ln_1.weight',
 'module.model.visual.transformer.resblocks.4.ln_1.bias',
 'module.model.visual.transformer.resblocks.4.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.4.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.4.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.4.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.4.ln_2.weight',
 'module.model.visual.transformer.resblocks.4.ln_2.bias',
 'module.model.visual.transformer.resblocks.5.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.5.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.5.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.5.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.5.ln_1.weight',
 'module.model.visual.transformer.resblocks.5.ln_1.bias',
 'module.model.visual.transformer.resblocks.5.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.5.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.5.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.5.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.5.ln_2.weight',
 'module.model.visual.transformer.resblocks.5.ln_2.bias',
 'module.model.visual.transformer.resblocks.6.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.6.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.6.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.6.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.6.ln_1.weight',
 'module.model.visual.transformer.resblocks.6.ln_1.bias',
 'module.model.visual.transformer.resblocks.6.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.6.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.6.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.6.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.6.ln_2.weight',
 'module.model.visual.transformer.resblocks.6.ln_2.bias',
 'module.model.visual.transformer.resblocks.7.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.7.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.7.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.7.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.7.ln_1.weight',
 'module.model.visual.transformer.resblocks.7.ln_1.bias',
 'module.model.visual.transformer.resblocks.7.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.7.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.7.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.7.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.7.ln_2.weight',
 'module.model.visual.transformer.resblocks.7.ln_2.bias',
 'module.model.visual.transformer.resblocks.8.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.8.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.8.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.8.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.8.ln_1.weight',
 'module.model.visual.transformer.resblocks.8.ln_1.bias',
 'module.model.visual.transformer.resblocks.8.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.8.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.8.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.8.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.8.ln_2.weight',
 'module.model.visual.transformer.resblocks.8.ln_2.bias',
 'module.model.visual.transformer.resblocks.9.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.9.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.9.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.9.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.9.ln_1.weight',
 'module.model.visual.transformer.resblocks.9.ln_1.bias',
 'module.model.visual.transformer.resblocks.9.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.9.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.9.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.9.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.9.ln_2.weight',
 'module.model.visual.transformer.resblocks.9.ln_2.bias',
 'module.model.visual.transformer.resblocks.10.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.10.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.10.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.10.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.10.ln_1.weight',
 'module.model.visual.transformer.resblocks.10.ln_1.bias',
 'module.model.visual.transformer.resblocks.10.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.10.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.10.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.10.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.10.ln_2.weight',
 'module.model.visual.transformer.resblocks.10.ln_2.bias',
 'module.model.visual.transformer.resblocks.11.attn.in_proj_weight',
 'module.model.visual.transformer.resblocks.11.attn.in_proj_bias',
 'module.model.visual.transformer.resblocks.11.attn.out_proj.weight',
 'module.model.visual.transformer.resblocks.11.attn.out_proj.bias',
 'module.model.visual.transformer.resblocks.11.ln_1.weight',
 'module.model.visual.transformer.resblocks.11.ln_1.bias',
 'module.model.visual.transformer.resblocks.11.mlp.c_fc.weight',
 'module.model.visual.transformer.resblocks.11.mlp.c_fc.bias',
 'module.model.visual.transformer.resblocks.11.mlp.c_proj.weight',
 'module.model.visual.transformer.resblocks.11.mlp.c_proj.bias',
 'module.model.visual.transformer.resblocks.11.ln_2.weight',
 'module.model.visual.transformer.resblocks.11.ln_2.bias',
 'module.model.visual.ln_post.weight',
 'module.model.visual.ln_post.bias',
 'module.model.transformer.resblocks.0.attn.in_proj_weight',
 'module.model.transformer.resblocks.0.attn.in_proj_bias',
 'module.model.transformer.resblocks.0.attn.out_proj.weight',
 'module.model.transformer.resblocks.0.attn.out_proj.bias',
 'module.model.transformer.resblocks.0.ln_1.weight',
 'module.model.transformer.resblocks.0.ln_1.bias',
 'module.model.transformer.resblocks.0.mlp.c_fc.weight',
 'module.model.transformer.resblocks.0.mlp.c_fc.bias',
 'module.model.transformer.resblocks.0.mlp.c_proj.weight',
 'module.model.transformer.resblocks.0.mlp.c_proj.bias',
 'module.model.transformer.resblocks.0.ln_2.weight',
 'module.model.transformer.resblocks.0.ln_2.bias',
 'module.model.transformer.resblocks.1.attn.in_proj_weight',
 'module.model.transformer.resblocks.1.attn.in_proj_bias',
 'module.model.transformer.resblocks.1.attn.out_proj.weight',
 'module.model.transformer.resblocks.1.attn.out_proj.bias',
 'module.model.transformer.resblocks.1.ln_1.weight',
 'module.model.transformer.resblocks.1.ln_1.bias',
 'module.model.transformer.resblocks.1.mlp.c_fc.weight',
 'module.model.transformer.resblocks.1.mlp.c_fc.bias',
 'module.model.transformer.resblocks.1.mlp.c_proj.weight',
 'module.model.transformer.resblocks.1.mlp.c_proj.bias',
 'module.model.transformer.resblocks.1.ln_2.weight',
 'module.model.transformer.resblocks.1.ln_2.bias',
 'module.model.transformer.resblocks.2.attn.in_proj_weight',
 'module.model.transformer.resblocks.2.attn.in_proj_bias',
 'module.model.transformer.resblocks.2.attn.out_proj.weight',
 'module.model.transformer.resblocks.2.attn.out_proj.bias',
 'module.model.transformer.resblocks.2.ln_1.weight',
 'module.model.transformer.resblocks.2.ln_1.bias',
 'module.model.transformer.resblocks.2.mlp.c_fc.weight',
 'module.model.transformer.resblocks.2.mlp.c_fc.bias',
 'module.model.transformer.resblocks.2.mlp.c_proj.weight',
 'module.model.transformer.resblocks.2.mlp.c_proj.bias',
 'module.model.transformer.resblocks.2.ln_2.weight',
 'module.model.transformer.resblocks.2.ln_2.bias',
 'module.model.transformer.resblocks.3.attn.in_proj_weight',
 'module.model.transformer.resblocks.3.attn.in_proj_bias',
 'module.model.transformer.resblocks.3.attn.out_proj.weight',
 'module.model.transformer.resblocks.3.attn.out_proj.bias',
 'module.model.transformer.resblocks.3.ln_1.weight',
 'module.model.transformer.resblocks.3.ln_1.bias',
 'module.model.transformer.resblocks.3.mlp.c_fc.weight',
 'module.model.transformer.resblocks.3.mlp.c_fc.bias',
 'module.model.transformer.resblocks.3.mlp.c_proj.weight',
 'module.model.transformer.resblocks.3.mlp.c_proj.bias',
 'module.model.transformer.resblocks.3.ln_2.weight',
 'module.model.transformer.resblocks.3.ln_2.bias',
 'module.model.transformer.resblocks.4.attn.in_proj_weight',
 'module.model.transformer.resblocks.4.attn.in_proj_bias',
 'module.model.transformer.resblocks.4.attn.out_proj.weight',
 'module.model.transformer.resblocks.4.attn.out_proj.bias',
 'module.model.transformer.resblocks.4.ln_1.weight',
 'module.model.transformer.resblocks.4.ln_1.bias',
 'module.model.transformer.resblocks.4.mlp.c_fc.weight',
 'module.model.transformer.resblocks.4.mlp.c_fc.bias',
 'module.model.transformer.resblocks.4.mlp.c_proj.weight',
 'module.model.transformer.resblocks.4.mlp.c_proj.bias',
 'module.model.transformer.resblocks.4.ln_2.weight',
 'module.model.transformer.resblocks.4.ln_2.bias',
 'module.model.transformer.resblocks.5.attn.in_proj_weight',
 'module.model.transformer.resblocks.5.attn.in_proj_bias',
 'module.model.transformer.resblocks.5.attn.out_proj.weight',
 'module.model.transformer.resblocks.5.attn.out_proj.bias',
 'module.model.transformer.resblocks.5.ln_1.weight',
 'module.model.transformer.resblocks.5.ln_1.bias',
 'module.model.transformer.resblocks.5.mlp.c_fc.weight',
 'module.model.transformer.resblocks.5.mlp.c_fc.bias',
 'module.model.transformer.resblocks.5.mlp.c_proj.weight',
 'module.model.transformer.resblocks.5.mlp.c_proj.bias',
 'module.model.transformer.resblocks.5.ln_2.weight',
 'module.model.transformer.resblocks.5.ln_2.bias',
 'module.model.transformer.resblocks.6.attn.in_proj_weight',
 'module.model.transformer.resblocks.6.attn.in_proj_bias',
 'module.model.transformer.resblocks.6.attn.out_proj.weight',
 'module.model.transformer.resblocks.6.attn.out_proj.bias',
 'module.model.transformer.resblocks.6.ln_1.weight',
 'module.model.transformer.resblocks.6.ln_1.bias',
 'module.model.transformer.resblocks.6.mlp.c_fc.weight',
 'module.model.transformer.resblocks.6.mlp.c_fc.bias',
 'module.model.transformer.resblocks.6.mlp.c_proj.weight',
 'module.model.transformer.resblocks.6.mlp.c_proj.bias',
 'module.model.transformer.resblocks.6.ln_2.weight',
 'module.model.transformer.resblocks.6.ln_2.bias',
 'module.model.transformer.resblocks.7.attn.in_proj_weight',
 'module.model.transformer.resblocks.7.attn.in_proj_bias',
 'module.model.transformer.resblocks.7.attn.out_proj.weight',
 'module.model.transformer.resblocks.7.attn.out_proj.bias',
 'module.model.transformer.resblocks.7.ln_1.weight',
 'module.model.transformer.resblocks.7.ln_1.bias',
 'module.model.transformer.resblocks.7.mlp.c_fc.weight',
 'module.model.transformer.resblocks.7.mlp.c_fc.bias',
 'module.model.transformer.resblocks.7.mlp.c_proj.weight',
 'module.model.transformer.resblocks.7.mlp.c_proj.bias',
 'module.model.transformer.resblocks.7.ln_2.weight',
 'module.model.transformer.resblocks.7.ln_2.bias',
 'module.model.transformer.resblocks.8.attn.in_proj_weight',
 'module.model.transformer.resblocks.8.attn.in_proj_bias',
 'module.model.transformer.resblocks.8.attn.out_proj.weight',
 'module.model.transformer.resblocks.8.attn.out_proj.bias',
 'module.model.transformer.resblocks.8.ln_1.weight',
 'module.model.transformer.resblocks.8.ln_1.bias',
 'module.model.transformer.resblocks.8.mlp.c_fc.weight',
 'module.model.transformer.resblocks.8.mlp.c_fc.bias',
 'module.model.transformer.resblocks.8.mlp.c_proj.weight',
 'module.model.transformer.resblocks.8.mlp.c_proj.bias',
 'module.model.transformer.resblocks.8.ln_2.weight',
 'module.model.transformer.resblocks.8.ln_2.bias',
 'module.model.transformer.resblocks.9.attn.in_proj_weight',
 'module.model.transformer.resblocks.9.attn.in_proj_bias',
 'module.model.transformer.resblocks.9.attn.out_proj.weight',
 'module.model.transformer.resblocks.9.attn.out_proj.bias',
 'module.model.transformer.resblocks.9.ln_1.weight',
 'module.model.transformer.resblocks.9.ln_1.bias',
 'module.model.transformer.resblocks.9.mlp.c_fc.weight',
 'module.model.transformer.resblocks.9.mlp.c_fc.bias',
 'module.model.transformer.resblocks.9.mlp.c_proj.weight',
 'module.model.transformer.resblocks.9.mlp.c_proj.bias',
 'module.model.transformer.resblocks.9.ln_2.weight',
 'module.model.transformer.resblocks.9.ln_2.bias',
 'module.model.transformer.resblocks.10.attn.in_proj_weight',
 'module.model.transformer.resblocks.10.attn.in_proj_bias',
 'module.model.transformer.resblocks.10.attn.out_proj.weight',
 'module.model.transformer.resblocks.10.attn.out_proj.bias',
 'module.model.transformer.resblocks.10.ln_1.weight',
 'module.model.transformer.resblocks.10.ln_1.bias',
 'module.model.transformer.resblocks.10.mlp.c_fc.weight',
 'module.model.transformer.resblocks.10.mlp.c_fc.bias',
 'module.model.transformer.resblocks.10.mlp.c_proj.weight',
 'module.model.transformer.resblocks.10.mlp.c_proj.bias',
 'module.model.transformer.resblocks.10.ln_2.weight',
 'module.model.transformer.resblocks.10.ln_2.bias',
 'module.model.transformer.resblocks.11.attn.in_proj_weight',
 'module.model.transformer.resblocks.11.attn.in_proj_bias',
 'module.model.transformer.resblocks.11.attn.out_proj.weight',
 'module.model.transformer.resblocks.11.attn.out_proj.bias',
 'module.model.transformer.resblocks.11.ln_1.weight',
 'module.model.transformer.resblocks.11.ln_1.bias',
 'module.model.transformer.resblocks.11.mlp.c_fc.weight',
 'module.model.transformer.resblocks.11.mlp.c_fc.bias',
 'module.model.transformer.resblocks.11.mlp.c_proj.weight',
 'module.model.transformer.resblocks.11.mlp.c_proj.bias',
 'module.model.transformer.resblocks.11.ln_2.weight',
 'module.model.transformer.resblocks.11.ln_2.bias',
 'module.model.token_embedding.weight',
 'module.model.ln_final.weight',
 'module.model.ln_final.bias',
 'module.projector_v.0.weight',
 'module.projector_v.2.weight',
 'module.projector_t.0.weight',
 'module.projector_t.2.weight']
[11/29 23:18:37][INFO] misc.py:  185: Model:
DistributedDataParallel(
  (module): TemporalClipVideo(
    (model): WCLIP(
      (visual): TemporalVisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TSTransformer(
          (resblocks): Sequential(
            (0): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (raw_model): WCLIP(
      (visual): TemporalVisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TSTransformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (projector_v): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): GELU()
      (2): Linear(in_features=512, out_features=512, bias=False)
    )
    (projector_t): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): GELU()
      (2): Linear(in_features=512, out_features=512, bias=False)
    )
  )
)
[11/29 23:18:37][INFO] misc.py:  187: Params: 300,290,050
[11/29 23:18:37][INFO] misc.py:  188: Mem: 1.6999154090881348 MB
[11/29 23:18:37][INFO] misc.py:  197: nvidia-smi
Fri Nov 29 23:18:37 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  Off |
| 42%   24C    P2    59W / 450W |   7438MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:23:00.0 Off |                  Off |
| 41%   25C    P2    57W / 450W |   5062MiB / 24564MiB |      4%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA GeForce ...  Off  | 00000000:41:00.0 Off |                  Off |
| 41%   27C    P2    59W / 450W |   5062MiB / 24564MiB |     11%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA GeForce ...  Off  | 00000000:61:00.0 Off |                  Off |
| 40%   25C    P2    60W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA GeForce ...  Off  | 00000000:81:00.0 Off |                  Off |
| 42%   25C    P2    60W / 450W |   5062MiB / 24564MiB |      9%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA GeForce ...  Off  | 00000000:A1:00.0 Off |                  Off |
| 41%   25C    P2    61W / 450W |   5062MiB / 24564MiB |      6%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA GeForce ...  Off  | 00000000:C1:00.0 Off |                  Off |
| 42%   25C    P2    52W / 450W |   5062MiB / 24564MiB |     12%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA GeForce ...  Off  | 00000000:E1:00.0 Off |                  Off |
| 39%   41C    P2   151W / 450W |   8894MiB / 24564MiB |     71%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    0   N/A  N/A   1543331      C   ...nda3/envs/py39/bin/python     2376MiB |
|    0   N/A  N/A   1572135      C   .../envs/slowfast/bin/python     5054MiB |
|    1   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    1   N/A  N/A   1572136      C   .../envs/slowfast/bin/python     5054MiB |
|    2   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    2   N/A  N/A   1572137      C   .../envs/slowfast/bin/python     5054MiB |
|    3   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    3   N/A  N/A   1572138      C   .../envs/slowfast/bin/python     5054MiB |
|    4   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    4   N/A  N/A   1572139      C   .../envs/slowfast/bin/python     5054MiB |
|    5   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    5   N/A  N/A   1572141      C   .../envs/slowfast/bin/python     5054MiB |
|    6   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    6   N/A  N/A   1572143      C   .../envs/slowfast/bin/python     5054MiB |
|    7   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    7   N/A  N/A   1572145      C   .../envs/slowfast/bin/python     5054MiB |
|    7   N/A  N/A   1731087      C   python                           3832MiB |
+-----------------------------------------------------------------------------+
bn 0, non bn 107, zero 199, no grad 302
[11/29 23:18:38][INFO] train_net.py: 1038: Load from last checkpoint.
[11/29 23:18:38][INFO] checkpoint.py:  222: Loading network weights from /mnt/SSD8T/home/huangwei/projects/FROSTER/checkpoints/basetraining/B2N_hmdb51_froster/checkpoints/checkpoint_epoch_00012.pyth.
missing keys: []
unexpected keys: []
[11/29 23:18:40][INFO] kinetics.py:   94: Constructing Kinetics train...
path: ---------------------------- /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb/train.csv
[11/29 23:18:40][INFO] kinetics.py:  172: Constructing kinetics dataloader (size: 416 skip_rows 0) from /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb/train.csv 
[11/29 23:18:40][INFO] kinetics.py:   94: Constructing Kinetics val...
path: ---------------------------- /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb/val.csv
[11/29 23:18:40][INFO] kinetics.py:  172: Constructing kinetics dataloader (size: 780 skip_rows 0) from /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb/val.csv 
[11/29 23:18:40][INFO] kinetics.py:   94: Constructing Kinetics train...
path: ---------------------------- /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb/train.csv
[11/29 23:18:40][INFO] kinetics.py:  172: Constructing kinetics dataloader (size: 416 skip_rows 0) from /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb/train.csv 
[11/29 23:18:40][INFO] train_net.py: 1129: Start epoch: 13
[11/29 23:19:07][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "13/12", "eta": "0:00:05", "gpu_mem": "3.54G", "iter": "10/25", "time_diff": 0.37389538, "top1_err": 23.43750000, "top5_err": 4.68750000}
[11/29 23:19:11][INFO] logging.py:   99: json_stats: {"_type": "val_iter", "epoch": "13/12", "eta": "0:00:01", "gpu_mem": "3.54G", "iter": "20/25", "time_diff": 0.30920687, "top1_err": 28.12500000, "top5_err": 3.12500000}
[11/29 23:19:14][INFO] logging.py:   99: json_stats: {"RAM": "105.35/251.74G", "_type": "val_epoch", "epoch": "13/12", "gpu_mem": "3.54G", "min_top1_err": 27.04081633, "min_top5_err": 4.97448980, "time_diff": 1.81647078, "top1_err": 27.04081633, "top5_err": 4.97448980}
[11/29 23:19:14][INFO] train_net.py: 1438: training done: _p300.29_f10.00 _t0.00_m3.54 _a72.96 Top5 Acc: 95.03 MEM: 3.54 f: 10.0000
[11/29 23:19:20][INFO] test_net.py:  294: Test with config:
[11/29 23:19:20][INFO] test_net.py:  295: AUG:
  AA_TYPE: rand-m7-n4-mstd0.5-inc1
  COLOR_JITTER: 0.4
  ENABLE: False
  GEN_MASK_LOADER: False
  INTERPOLATION: bicubic
  MASK_FRAMES: False
  MASK_RATIO: 0.0
  MASK_TUBE: False
  MASK_WINDOW_SIZE: [8, 7, 7]
  MAX_MASK_PATCHES_PER_BLOCK: None
  NUM_SAMPLE: 1
  RE_COUNT: 1
  RE_MODE: pixel
  RE_PROB: 0.25
  RE_SPLIT: False
AVA:
  ANNOTATION_DIR: /mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/
  BGR: False
  DETECTION_SCORE_THRESH: 0.9
  EXCLUSION_FILE: ava_val_excluded_timestamps_v2.2.csv
  FRAME_DIR: /mnt/fair-flash3-east/ava_trainval_frames.img/
  FRAME_LIST_DIR: /mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/
  FULL_TEST_ON_VAL: False
  GROUNDTRUTH_FILE: ava_val_v2.2.csv
  IMG_PROC_BACKEND: cv2
  LABEL_MAP_FILE: ava_action_list_v2.2_for_activitynet_2019.pbtxt
  TEST_FORCE_FLIP: False
  TEST_LISTS: ['val.csv']
  TEST_PREDICT_BOX_LISTS: ['ava_val_predicted_boxes.csv']
  TRAIN_GT_BOX_LISTS: ['ava_train_v2.2.csv']
  TRAIN_LISTS: ['train.csv']
  TRAIN_PCA_JITTER_ONLY: True
  TRAIN_PREDICT_BOX_LISTS: []
  TRAIN_USE_COLOR_AUGMENTATION: False
BENCHMARK:
  LOG_PERIOD: 100
  NUM_EPOCHS: 5
  SHUFFLE: True
BN:
  GLOBAL_SYNC: False
  NORM_TYPE: batchnorm
  NUM_BATCHES_PRECISE: 200
  NUM_SPLITS: 1
  NUM_SYNC_DEVICES: 1
  USE_PRECISE_STATS: False
  WEIGHT_DECAY: 0.0
CONTRASTIVE:
  BN_MLP: False
  BN_SYNC_MLP: False
  DELTA_CLIPS_MAX: inf
  DELTA_CLIPS_MIN: -inf
  DIM: 128
  INTERP_MEMORY: False
  KNN_ON: True
  LENGTH: 239975
  LOCAL_SHUFFLE_BN: True
  MEM_TYPE: 1d
  MLP_DIM: 2048
  MOCO_MULTI_VIEW_QUEUE: False
  MOMENTUM: 0.5
  MOMENTUM_ANNEALING: False
  NUM_CLASSES_DOWNSTREAM: 400
  NUM_MLP_LAYERS: 1
  PREDICTOR_DEPTHS: []
  QUEUE_LEN: 65536
  SEQUENTIAL: False
  SIMCLR_DIST_ON: True
  SWAV_QEUE_LEN: 0
  T: 0.07
  TYPE: mem
DATA:
  COLOR_RND_GRAYSCALE: 0.0
  DECODING_BACKEND: pyav
  DECODING_SHORT_SIZE: 256
  DUMMY_LOAD: False
  ENSEMBLE_METHOD: sum
  IN22K_TRAINVAL: False
  IN22k_VAL_IN1K: 
  INDEX_LABEL_MAPPING_FILE: /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb/train_rephrased.json
  INPUT_CHANNEL_NUM: [3]
  INV_UNIFORM_SAMPLE: False
  IN_VAL_CROP_RATIO: 0.875
  LOADER_CHUNK_OVERALL_SIZE: 0
  LOADER_CHUNK_SIZE: 0
  MEAN: [0.48145466, 0.4578275, 0.40821073]
  MULTI_LABEL: False
  NUM_FRAMES: 8
  PATH_LABEL_SEPARATOR: ,
  PATH_PREFIX: /mnt/SSD8T/home/huangwei/projects/FROSTER/data/hmdb51
  PATH_TO_DATA_DIR: /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb
  PATH_TO_PRELOAD_IMDB: 
  RANDOM_FLIP: True
  REVERSE_INPUT_CHANNEL: False
  SAMPLING_RATE: 16
  SKIP_ROWS: 0
  SSL_BLUR_SIGMA_MAX: [0.0, 2.0]
  SSL_BLUR_SIGMA_MIN: [0.0, 0.1]
  SSL_COLOR_BRI_CON_SAT: [0.4, 0.4, 0.4]
  SSL_COLOR_HUE: 0.1
  SSL_COLOR_JITTER: False
  SSL_MOCOV2_AUG: False
  STD: [0.26862954, 0.26130258, 0.27577711]
  TARGET_FPS: 30
  TEST_CROP_SIZE: 224
  TIME_DIFF_PROB: 0.0
  TRAIN_CROP_NUM_SPATIAL: 1
  TRAIN_CROP_NUM_TEMPORAL: 1
  TRAIN_CROP_SIZE: 224
  TRAIN_JITTER_ASPECT_RELATIVE: []
  TRAIN_JITTER_FPS: 0.0
  TRAIN_JITTER_MOTION_SHIFT: False
  TRAIN_JITTER_SCALES: [224, 256]
  TRAIN_JITTER_SCALES_RELATIVE: []
  TRAIN_PCA_EIGVAL: [0.225, 0.224, 0.229]
  TRAIN_PCA_EIGVEC: [[-0.5675, 0.7192, 0.4009], [-0.5808, -0.0045, -0.814], [-0.5836, -0.6948, 0.4203]]
  USE_OFFSET_SAMPLING: True
DATA_LOADER:
  ENABLE_MULTI_THREAD_DECODE: False
  NUM_WORKERS: 8
  PIN_MEMORY: True
DEMO:
  BUFFER_SIZE: 0
  CLIP_VIS_SIZE: 10
  COMMON_CLASS_NAMES: ['watch (a person)', 'talk to (e.g., self, a person, a group)', 'listen to (a person)', 'touch (an object)', 'carry/hold (an object)', 'walk', 'sit', 'lie/sleep', 'bend/bow (at the waist)']
  COMMON_CLASS_THRES: 0.7
  DETECTRON2_CFG: COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml
  DETECTRON2_THRESH: 0.9
  DETECTRON2_WEIGHTS: detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl
  DISPLAY_HEIGHT: 0
  DISPLAY_WIDTH: 0
  ENABLE: False
  FPS: 30
  GT_BOXES: 
  INPUT_FORMAT: BGR
  INPUT_VIDEO: 
  LABEL_FILE_PATH: 
  NUM_CLIPS_SKIP: 0
  NUM_VIS_INSTANCES: 2
  OUTPUT_FILE: 
  OUTPUT_FPS: -1
  PREDS_BOXES: 
  SLOWMO: 1
  STARTING_SECOND: 900
  THREAD_ENABLE: False
  UNCOMMON_CLASS_THRES: 0.3
  VIS_MODE: thres
  WEBCAM: -1
DETECTION:
  ALIGNED: True
  ENABLE: False
  ROI_XFORM_RESOLUTION: 7
  SPATIAL_SCALE_FACTOR: 16
DIST_BACKEND: nccl
IMAGENET_SIMPLELABEL_PATH: None
LOG_MODEL_INFO: True
LOG_PERIOD: 10
MASK:
  DECODER_DEPTH: 0
  DECODER_EMBED_DIM: 512
  DECODER_SEP_POS_EMBED: False
  DEC_KV_KERNEL: []
  DEC_KV_STRIDE: []
  ENABLE: False
  HEAD_TYPE: separate
  MAE_ON: False
  MAE_RND_MASK: False
  NORM_PRED_PIXEL: True
  PER_FRAME_MASKING: False
  PRED_HOG: False
  PRETRAIN_DEPTH: [15]
  SCALE_INIT_BY_DEPTH: False
  TIME_STRIDE_LOSS: True
MIXUP:
  ALPHA: 0.8
  CUTMIX_ALPHA: 1.0
  ENABLE: False
  LABEL_SMOOTH_VALUE: 0.1
  PROB: 1.0
  SWITCH_PROB: 0.5
MODEL:
  ACT_CHECKPOINT: False
  ADAPT_FINETUNE_FACTOR: 1.0
  ARCH: vitb16
  CLS_LOSS_RATIO: 1.0
  CONTEXT_LENGTH: 77
  DEFAULT_FINETUNE_FACTOR: 1.0
  DETACH_FINAL_FC: False
  DISTILLATION_RATIO: 2.0
  DROPCONNECT_RATE: 0.0
  DROPOUT_RATE: 0.5
  ENSEMBLE_PRED: False
  ENSEMBLE_RAWMODEL_RATIO: 0.0
  EXPERT_FINETUNE_FACTOR: 1.0
  EXPERT_INSERT_LAYERS: [10, 11]
  FC_INIT_STD: 0.01
  FINETUNE_FACTOR: 1.0
  FP16_ALLREDUCE: False
  FROZEN_BN: False
  HEAD_ACT: softmax
  KEEP_RAW_MODEL: True
  LOSS_FREQ_TYPE: mse
  LOSS_FUNC: soft_cross_entropy
  MLP_FINETUNE_FACTOR: 1.0
  MODEL_NAME: TemporalClipVideo
  MULTI_PATHWAY_ARCH: ['slowfast']
  NUM_CLASSES: 26
  NUM_EXPERTS: 0
  PROMPT_NUM: 1
  RAW_MODEL_DISTILLATION: True
  RECORD_ROUTING: False
  ROUTING_FINETUNE_FACTOR: 1.0
  ROUTING_FREQUENCE_CONSTRAIN: 0.5
  ROUTING_FREQ_CONS_FACTOR: 1.0
  ROUTING_TYPE: patch-level
  SINGLE_PATHWAY_ARCH: ['2d', 'c2d', 'i3d', 'slow', 'x3d', 'mvit', 'maskmvit', 'vitb32', 'vitb16', 'vitl14']
  STATIC_GRAPH: False
  TEMPORAL_MODELING_TYPE: expand_temporal_view
  TEXT_PROMPT: False
  USE_CHECKPOINT: False
MULTIGRID:
  BN_BASE_SIZE: 8
  DEFAULT_B: 0
  DEFAULT_S: 0
  DEFAULT_T: 0
  EPOCH_FACTOR: 1.5
  EVAL_FREQ: 3
  LONG_CYCLE: False
  LONG_CYCLE_FACTORS: [(0.25, 0.7071067811865476), (0.5, 0.7071067811865476), (0.5, 1), (1, 1)]
  LONG_CYCLE_SAMPLING_RATE: 0
  SHORT_CYCLE: False
  SHORT_CYCLE_FACTORS: [0.5, 0.7071067811865476]
MVIT:
  CLS_EMBED_ON: True
  DEPTH: 16
  DIM_MUL: []
  DIM_MUL_IN_ATT: False
  DROPOUT_RATE: 0.0
  DROPPATH_RATE: 0.1
  EMBED_DIM: 96
  HEAD_INIT_SCALE: 1.0
  HEAD_MUL: []
  LAYER_SCALE_INIT_VALUE: 0.0
  MLP_RATIO: 4.0
  MODE: conv
  NORM: layernorm
  NORM_STEM: False
  NUM_HEADS: 1
  PATCH_2D: False
  PATCH_KERNEL: [3, 7, 7]
  PATCH_PADDING: [2, 4, 4]
  PATCH_STRIDE: [2, 4, 4]
  POOL_FIRST: False
  POOL_KVQ_KERNEL: None
  POOL_KV_STRIDE: []
  POOL_KV_STRIDE_ADAPTIVE: None
  POOL_Q_STRIDE: []
  QKV_BIAS: True
  REL_POS_SPATIAL: False
  REL_POS_TEMPORAL: False
  REL_POS_ZERO_INIT: False
  RESIDUAL_POOLING: False
  REV:
    BUFFER_LAYERS: []
    ENABLE: False
    PRE_Q_FUSION: avg
    RESPATH_FUSE: concat
    RES_PATH: conv
  SEPARATE_QKV: False
  SEP_POS_EMBED: False
  USE_ABS_POS: True
  USE_FIXED_SINCOS_POS: False
  USE_MEAN_POOLING: False
  ZERO_DECAY_POS_CLS: True
NONLOCAL:
  GROUP: [[1], [1], [1], [1]]
  INSTANTIATION: dot_product
  LOCATION: [[[]], [[]], [[]], [[]]]
  POOL: [[[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]], [[1, 2, 2], [1, 2, 2]]]
NUM_GPUS: 8
NUM_SHARDS: 1
OUTPUT_DIR: /mnt/SSD8T/home/huangwei/projects/FROSTER/checkpoints/basetraining/B2N_hmdb51_froster
RESNET:
  DEPTH: 50
  INPLACE_RELU: True
  NUM_BLOCK_TEMP_KERNEL: [[3], [4], [6], [3]]
  NUM_GROUPS: 1
  SPATIAL_DILATIONS: [[1], [1], [1], [1]]
  SPATIAL_STRIDES: [[1], [2], [2], [2]]
  STRIDE_1X1: False
  TRANS_FUNC: bottleneck_transform
  WIDTH_PER_GROUP: 64
  ZERO_INIT_FINAL_BN: False
  ZERO_INIT_FINAL_CONV: False
RNG_SEED: 0
SHARD_ID: 0
SLOWFAST:
  ALPHA: 8
  BETA_INV: 8
  FUSION_CONV_CHANNEL_RATIO: 2
  FUSION_KERNEL_SZ: 5
SOLVER:
  BASE_LR: 3.33e-06
  BASE_LR_SCALE_NUM_SHARDS: True
  BETAS: (0.9, 0.999)
  CLIP_GRAD_L2NORM: 1.0
  CLIP_GRAD_VAL: None
  COSINE_AFTER_WARMUP: True
  COSINE_END_LR: 3.33e-08
  COSINE_RESTART_EPOCH: 0
  DAMPENING: 0.0
  GAMMA: 0.1
  LARS_ON: False
  LAYER_DECAY: 1.0
  LRS: []
  LR_POLICY: cosine
  MAX_EPOCH: 12
  MOMENTUM: 0.9
  NESTEROV: True
  OPTIMIZING_METHOD: adamw
  STEPS: []
  STEP_SIZE: 1
  WARMUP_EPOCHS: 2.0
  WARMUP_FACTOR: 0.1
  WARMUP_START_LR: 3.33e-08
  WEIGHT_DECAY: 0.01
  ZERO_WD_1D_PARAM: True
TASK: 
TENSORBOARD:
  CATEGORIES_PATH: 
  CLASS_NAMES_PATH: 
  CONFUSION_MATRIX:
    ENABLE: False
    FIGSIZE: [8, 8]
    SUBSET_PATH: 
  ENABLE: False
  HISTOGRAM:
    ENABLE: False
    FIGSIZE: [8, 8]
    SUBSET_PATH: 
    TOPK: 10
  LOG_DIR: 
  MODEL_VIS:
    ACTIVATIONS: False
    COLORMAP: Pastel2
    ENABLE: False
    GRAD_CAM:
      COLORMAP: viridis
      ENABLE: True
      LAYER_LIST: []
      USE_TRUE_LABEL: False
    INPUT_VIDEO: False
    LAYER_LIST: []
    MODEL_WEIGHTS: False
    TOPK_PREDS: 1
  PREDICTIONS_PATH: 
  WRONG_PRED_VIS:
    ENABLE: False
    SUBSET_PATH: 
    TAG: Incorrectly classified videos.
TEST:
  BATCH_SIZE: 240
  CHECKPOINT_FILE_PATH: 
  CHECKPOINT_TYPE: pytorch
  CLIP_ORI_PATH: None
  CUSTOM_LOAD: False
  CUSTOM_LOAD_FILE: None
  DATASET: kinetics
  ENABLE: True
  NUM_ENSEMBLE_VIEWS: 3
  NUM_SPATIAL_CROPS: 1
  NUM_TEMPORAL_CLIPS: [3]
  OPENSET: False
  PATCHING_MODEL: False
  PATCHING_RATIO: 0.5
  SAVE_RESULTS_PATH: 
  UPDATE_STATE: False
TEST_FILE: test.csv
TRAIN:
  ADAPT_ZS_CONS_RATIO: False
  AUTO_RESUME: True
  BATCH_SIZE: 32
  CHECKPOINT_CLEAR_NAME_PATTERN: ()
  CHECKPOINT_EPOCH_RESET: False
  CHECKPOINT_FILE_PATH: 
  CHECKPOINT_INFLATE: False
  CHECKPOINT_IN_INIT: False
  CHECKPOINT_PERIOD: 1
  CHECKPOINT_TYPE: pytorch
  CLIP_ORI_PATH: /mnt/SSD8T/home/huangwei/.cache/clip/ViT-B-16.pt
  CUSTOM_LOAD: False
  CUSTOM_LOAD_FILE: None
  DATASET: kinetics
  ENABLE: True
  EVAL_PERIOD: 1
  EWC_CONSTRAIN_RATIO: 1.0
  EWC_IDENTITY_FISHER: False
  EWC_IGNORE_LOGIT_SCALE: False
  EWC_LOAD_FILE: None
  EWC_SET: False
  KILL_LOSS_EXPLOSION_FACTOR: 0.0
  LINEAR_CONNECT_CLIMB: False
  LINEAR_CONNECT_LOSS_RATIO: 0.0
  LINEAR_CONNECT_SAMPLE: True
  LINEAR_CONNECT_SAMPLE_L: 0.4
  LINEAR_CONNECT_SAMPLE_R: 0.6
  MIXED_PRECISION: True
  ZS_CONS: False
  ZS_CONS_RATIO: 0.8
  ZS_INIT_CONS: False
  ZS_RESTART_CONS: False
  ZS_RESTART_EPOCH: -1
TRAIN_FILE: train.csv
TUNE_HEAD: False
VAL_FILE: val.csv
VAL_MODE: False
VIS_MASK:
  ENABLE: False
X3D:
  BN_LIN5: False
  BOTTLENECK_FACTOR: 1.0
  CHANNELWISE_3x3x3: True
  DEPTH_FACTOR: 1.0
  DIM_C1: 12
  DIM_C5: 2048
  SCALE_RES2: False
  WIDTH_FACTOR: 1.0
[11/29 23:19:26][INFO] temporalclip_video_model.py:  604: load pretrained CLIP:<All keys matched successfully>
[11/29 23:19:34][INFO] temporalclip_video_model.py:  604: load pretrained CLIP:<All keys matched successfully>
[11/29 23:19:36][INFO] checkpoint.py:  222: Loading network weights from /mnt/SSD8T/home/huangwei/projects/FROSTER/checkpoints/basetraining/B2N_hmdb51_froster/checkpoints/checkpoint_epoch_00012.pyth.
missing keys: []
unexpected keys: []
[11/29 23:19:38][INFO] misc.py:  185: Model:
DistributedDataParallel(
  (module): TemporalClipVideo(
    (model): WCLIP(
      (visual): TemporalVisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TSTransformer(
          (resblocks): Sequential(
            (0): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (raw_model): WCLIP(
      (visual): TemporalVisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TSTransformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (projector_v): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): GELU()
      (2): Linear(in_features=512, out_features=512, bias=False)
    )
    (projector_t): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): GELU()
      (2): Linear(in_features=512, out_features=512, bias=False)
    )
  )
)
[11/29 23:19:38][INFO] misc.py:  187: Params: 300,290,050
[11/29 23:19:38][INFO] misc.py:  188: Mem: 1.6999154090881348 MB
[11/29 23:19:38][INFO] misc.py:  197: nvidia-smi
Fri Nov 29 23:19:38 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  Off |
| 42%   26C    P2    67W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:23:00.0 Off |                  Off |
| 41%   27C    P2    65W / 450W |   5062MiB / 24564MiB |      2%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA GeForce ...  Off  | 00000000:41:00.0 Off |                  Off |
| 41%   29C    P2    70W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA GeForce ...  Off  | 00000000:61:00.0 Off |                  Off |
| 40%   28C    P2    67W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA GeForce ...  Off  | 00000000:81:00.0 Off |                  Off |
| 42%   28C    P2    64W / 450W |   5062MiB / 24564MiB |      4%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA GeForce ...  Off  | 00000000:A1:00.0 Off |                  Off |
| 41%   27C    P2    68W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA GeForce ...  Off  | 00000000:C1:00.0 Off |                  Off |
| 42%   27C    P2    59W / 450W |   5062MiB / 24564MiB |      3%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA GeForce ...  Off  | 00000000:E1:00.0 Off |                  Off |
| 40%   35C    P2   125W / 450W |   8894MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    0   N/A  N/A   1582568      C   .../envs/slowfast/bin/python     5054MiB |
|    1   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    1   N/A  N/A   1582569      C   .../envs/slowfast/bin/python     5054MiB |
|    2   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    2   N/A  N/A   1582570      C   .../envs/slowfast/bin/python     5054MiB |
|    3   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    3   N/A  N/A   1582571      C   .../envs/slowfast/bin/python     5054MiB |
|    4   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    4   N/A  N/A   1582572      C   .../envs/slowfast/bin/python     5054MiB |
|    5   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    5   N/A  N/A   1582574      C   .../envs/slowfast/bin/python     5054MiB |
|    6   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    6   N/A  N/A   1582576      C   .../envs/slowfast/bin/python     5054MiB |
|    7   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    7   N/A  N/A   1582650      C   .../envs/slowfast/bin/python     5054MiB |
|    7   N/A  N/A   1731087      C   python                           3832MiB |
+-----------------------------------------------------------------------------+
[11/29 23:19:39][INFO] misc.py:  185: Model:
DistributedDataParallel(
  (module): TemporalClipVideo(
    (model): WCLIP(
      (visual): TemporalVisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TSTransformer(
          (resblocks): Sequential(
            (0): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): TimesAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (raw_model): WCLIP(
      (visual): TemporalVisionTransformer(
        (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (transformer): TSTransformer(
          (resblocks): Sequential(
            (0): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (1): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (2): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (3): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (4): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (5): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (6): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (7): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (8): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (9): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (10): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
            (11): ResidualAttentionBlock(
              (attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
              )
              (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Sequential(
                (c_fc): Linear(in_features=768, out_features=3072, bias=True)
                (gelu): QuickGELU()
                (c_proj): Linear(in_features=3072, out_features=768, bias=True)
              )
              (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
        (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (transformer): Transformer(
        (resblocks): Sequential(
          (0): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (1): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (2): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (3): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (4): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (5): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (6): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (7): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (8): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (9): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (10): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
          (11): ResidualAttentionBlock(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=512, out_features=2048, bias=True)
              (gelu): QuickGELU()
              (c_proj): Linear(in_features=2048, out_features=512, bias=True)
            )
            (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (token_embedding): Embedding(49408, 512)
      (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (projector_v): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): GELU()
      (2): Linear(in_features=512, out_features=512, bias=False)
    )
    (projector_t): Sequential(
      (0): Linear(in_features=512, out_features=512, bias=False)
      (1): GELU()
      (2): Linear(in_features=512, out_features=512, bias=False)
    )
  )
)
[11/29 23:19:39][INFO] misc.py:  187: Params: 300,290,050
[11/29 23:19:39][INFO] misc.py:  188: Mem: 1.6999154090881348 MB
[11/29 23:19:39][INFO] misc.py:  197: nvidia-smi
Fri Nov 29 23:19:39 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  Off |
| 42%   26C    P2    64W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce ...  Off  | 00000000:23:00.0 Off |                  Off |
| 41%   27C    P2    61W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA GeForce ...  Off  | 00000000:41:00.0 Off |                  Off |
| 41%   28C    P2    70W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA GeForce ...  Off  | 00000000:61:00.0 Off |                  Off |
| 40%   27C    P2    58W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA GeForce ...  Off  | 00000000:81:00.0 Off |                  Off |
| 42%   27C    P2    61W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA GeForce ...  Off  | 00000000:A1:00.0 Off |                  Off |
| 41%   26C    P2    67W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA GeForce ...  Off  | 00000000:C1:00.0 Off |                  Off |
| 42%   27C    P2    56W / 450W |   5062MiB / 24564MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA GeForce ...  Off  | 00000000:E1:00.0 Off |                  Off |
| 39%   36C    P2   128W / 450W |   8894MiB / 24564MiB |     44%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    0   N/A  N/A   1582568      C   .../envs/slowfast/bin/python     5054MiB |
|    1   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    1   N/A  N/A   1582569      C   .../envs/slowfast/bin/python     5054MiB |
|    2   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    2   N/A  N/A   1582570      C   .../envs/slowfast/bin/python     5054MiB |
|    3   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    3   N/A  N/A   1582571      C   .../envs/slowfast/bin/python     5054MiB |
|    4   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    4   N/A  N/A   1582572      C   .../envs/slowfast/bin/python     5054MiB |
|    5   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    5   N/A  N/A   1582574      C   .../envs/slowfast/bin/python     5054MiB |
|    6   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    6   N/A  N/A   1582576      C   .../envs/slowfast/bin/python     5054MiB |
|    7   N/A  N/A      2891      G   /usr/lib/xorg/Xorg                  4MiB |
|    7   N/A  N/A   1582650      C   .../envs/slowfast/bin/python     5054MiB |
|    7   N/A  N/A   1731087      C   python                           3832MiB |
+-----------------------------------------------------------------------------+
[11/29 23:19:40][INFO] kinetics.py:   94: Constructing Kinetics test...
path: ---------------------------- /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb/test.csv
[11/29 23:19:40][INFO] kinetics.py:  172: Constructing kinetics dataloader (size: 2250 skip_rows 0) from /mnt/SSD8T/home/huangwei/projects/FROSTER/zs_label_db/B2N_hmdb/test.csv 
[11/29 23:19:40][INFO] test_net.py:  409: Testing model for 10 iterations
type of preds: <class 'tuple'>, type of labels: <class 'torch.Tensor'>, type of video_idx: <class 'torch.Tensor'>
如果有任何一个是tuple，就说明发生了错误
preds = ([tensor([[25.3442, 24.5602, 23.5662, 23.6469, 21.5098, 21.0294, 23.7909, 21.3424,
         22.2757, 20.8440, 23.1416, 22.5783, 21.8578, 22.9057, 21.4377, 25.5427,
         24.3511, 26.2878, 19.2624, 24.4656, 24.6116, 23.2301, 22.1172, 25.3037,
         22.5984, 24.4107],
        [22.1058, 19.1452, 22.0145, 20.7249, 18.1714, 20.5808, 18.5989, 21.4855,
         21.5455, 18.2053, 18.6868, 20.4243, 26.0371, 19.1691, 19.2571, 23.6031,
         19.2803, 23.2504, 17.6164, 15.8898, 18.5146, 24.8354, 19.3808, 22.0726,
         21.3803, 22.3188],
        [14.4752, 19.2564, 19.4656, 21.8712, 20.5998, 19.1548, 19.7631, 22.5405,
         14.2187, 28.5050, 22.8731, 24.2654, 19.3435, 19.9549, 16.7413, 18.7875,
         26.3401, 20.9049, 21.4524, 17.4576, 19.4147, 18.5234, 24.4522, 19.0747,
         19.6100, 17.6163],
        [19.1892, 20.6301, 27.4474, 23.7886, 18.6745, 21.4773, 22.1393, 24.3724,
         22.4804, 24.6546, 29.4478, 25.5632, 19.6985, 20.1796, 18.2689, 23.6185,
         21.0830, 21.9462, 26.6487, 35.0079, 34.4920, 21.6643, 24.9436, 21.0907,
         23.5759, 19.9077],
        [17.4858, 25.9067, 20.2046, 23.6407, 23.0881, 18.8924, 18.8993, 21.1747,
         17.4249, 20.1689, 20.9718, 18.3558, 22.7972, 25.3019, 14.9805, 17.9245,
         27.7931, 25.3489, 16.4957, 18.5911, 19.0715, 17.8880, 23.8333, 20.7474,
         17.7268, 21.2378],
        [19.4861, 22.2070, 26.9918, 24.7294, 25.6648, 21.2734, 24.1052, 24.8074,
         19.7363, 20.2155, 22.4592, 22.9490, 24.8205, 26.0487, 19.2346, 23.1273,
         27.4412, 25.5625, 21.0920, 23.4040, 21.6965, 21.4184, 24.7829, 25.1569,
         22.6155, 23.7607],
        [16.4650, 16.1942, 19.3838, 19.1369, 16.6214, 17.9414, 15.1849, 20.7491,
         16.2715, 17.7215, 18.4273, 19.0209, 21.3264, 17.4403, 18.3217, 25.1873,
         16.2035, 19.5350, 19.9335, 16.4958, 19.6950, 22.1801, 18.9612, 16.6056,
         19.5229, 19.0173],
        [19.7152, 23.7590, 22.8947, 23.8597, 17.2001, 20.2930, 19.0821, 23.2923,
         17.5771, 26.1124, 26.1245, 24.5503, 21.0704, 22.0102, 18.1714, 21.5884,
         20.1112, 25.1509, 30.7402, 22.4147, 24.2366, 19.2259, 26.8148, 25.8083,
         22.1694, 21.4057],
        [20.6917, 22.7356, 24.8153, 21.6401, 18.1495, 21.3927, 20.7727, 23.7091,
         23.6478, 24.2964, 27.4954, 25.2730, 20.5407, 22.2203, 22.7711, 25.0900,
         20.3546, 24.2225, 24.6093, 28.4620, 30.3836, 28.2184, 27.5863, 25.1100,
         24.5315, 27.9251],
        [17.5824, 18.3014, 21.6414, 23.1316, 21.4400, 20.1701, 20.3891, 24.6889,
         18.5113, 29.7335, 24.5771, 25.8319, 21.5731, 20.9484, 17.2036, 20.1494,
         27.4413, 22.8181, 22.4622, 20.8613, 21.5542, 18.7033, 26.6794, 22.7852,
         22.0968, 20.4524],
        [19.1003, 20.2795, 24.7884, 18.8562, 17.2830, 17.9250, 14.8929, 19.8689,
         27.1806, 18.0825, 17.1397, 19.4307, 22.8424, 17.5211, 20.3165, 20.7308,
         18.5299, 23.3672, 19.1511, 17.7258, 19.1705, 25.6647, 19.7689, 22.8583,
         21.7265, 25.1160],
        [22.5915, 26.3518, 24.5708, 27.0911, 29.6687, 23.0642, 24.5628, 27.0576,
         19.1537, 25.7174, 24.5634, 23.3460, 23.3499, 28.5647, 20.9370, 22.5203,
         32.9646, 28.0595, 25.3915, 27.4297, 25.3545, 20.3304, 27.4121, 26.8831,
         24.4054, 22.9520],
        [19.1592, 29.0058, 23.1228, 29.4588, 27.0116, 25.7226, 24.1413, 28.6656,
         17.2147, 22.1736, 19.1576, 19.7939, 24.0035, 28.5004, 18.4320, 21.2371,
         28.4986, 30.9232, 21.1326, 22.6384, 19.7096, 21.1374, 20.9788, 24.9077,
         27.4781, 20.1768],
        [23.1097, 25.5460, 26.8002, 22.0545, 23.8296, 19.1598, 23.0205, 23.4437,
         24.3662, 25.3193, 28.0648, 24.7402, 21.8480, 25.5906, 21.1998, 23.1729,
         27.1576, 27.2096, 25.8386, 34.2003, 34.7062, 21.7483, 27.1209, 25.2017,
         23.1343, 24.2543],
        [25.1005, 18.0345, 27.5481, 25.7652, 20.5541, 20.8995, 22.2186, 23.2588,
         18.2539, 24.7703, 22.7520, 25.6054, 20.4531, 19.2699, 15.4567, 24.4632,
         23.6647, 21.8322, 25.7272, 24.4427, 26.2078, 19.9227, 27.8797, 19.2803,
         22.5579, 20.0269],
        [22.8092, 22.4151, 24.7113, 23.6329, 24.4465, 20.8709, 22.9702, 21.3308,
         22.1938, 20.4329, 21.3114, 22.2303, 22.9453, 23.3699, 20.5807, 24.2584,
         25.2729, 23.6595, 22.0964, 24.5686, 22.4665, 22.8021, 22.4005, 21.1295,
         21.0081, 22.1437],
        [17.7019, 21.6079, 22.0001, 24.0334, 20.9405, 22.1456, 21.4548, 26.7741,
         18.0012, 30.0178, 23.5651, 23.0033, 20.8405, 21.6658, 18.0258, 19.5936,
         24.5661, 24.2165, 22.3311, 20.5738, 19.1702, 18.2151, 25.0362, 25.3341,
         23.1559, 20.8950],
        [15.5109, 22.3770, 19.6545, 20.3380, 18.7597, 22.0506, 22.0872, 22.4150,
         14.9645, 18.6266, 18.4495, 17.5236, 20.2287, 21.4590, 17.0815, 20.0577,
         27.3772, 21.8524, 18.7391, 19.4238, 19.4028, 19.2098, 22.4725, 18.2123,
         20.1591, 20.3565],
        [20.6191, 23.7833, 26.3133, 25.1272, 23.4731, 22.7523, 22.1821, 25.2412,
         27.4132, 32.2248, 25.2479, 24.2801, 23.2807, 25.1945, 23.0350, 23.1676,
         23.8690, 27.1967, 25.1863, 23.2085, 21.8067, 26.3366, 27.7712, 27.2768,
         25.8687, 25.7242],
        [20.1015, 25.3618, 25.7225, 23.9089, 25.4523, 21.4610, 23.0752, 24.9554,
         17.8537, 24.9769, 21.5955, 21.6071, 21.7019, 25.4638, 20.6327, 21.0435,
         33.5303, 25.7655, 24.9281, 24.7266, 22.8422, 20.3145, 29.4298, 24.7175,
         22.0077, 25.6086],
        [17.2748, 18.5377, 21.0004, 21.0243, 18.1099, 18.0311, 17.8981, 18.6903,
         19.2417, 17.5003, 16.7760, 17.2609, 22.0386, 20.2156, 25.1197, 23.0179,
         18.2606, 21.3302, 16.6563, 15.0670, 14.6262, 24.4609, 16.4010, 18.8433,
         18.5164, 21.5568],
        [15.8694, 18.3953, 23.2102, 24.8364, 16.3037, 20.3619, 19.7290, 20.6289,
         24.0778, 18.4656, 18.6776, 18.3432, 18.5705, 15.3025, 19.7246, 20.8270,
         18.0054, 21.4740, 18.0034, 17.7011, 18.1017, 23.7227, 19.3241, 17.5679,
         23.0724, 21.9373],
        [20.9751, 19.6291, 21.3255, 21.5440, 20.3916, 18.8615, 17.2941, 21.6277,
         19.3332, 17.7188, 18.3720, 19.9856, 24.1997, 20.5868, 20.8979, 24.8796,
         21.1892, 25.3095, 17.2535, 16.8542, 19.9568, 24.2756, 20.9671, 23.6793,
         21.9753, 22.5660],
        [16.4605, 21.5315, 20.2164, 21.7084, 16.8743, 19.4128, 17.0862, 22.9110,
         17.4374, 29.7335, 23.7305, 23.7902, 17.6716, 18.6731, 17.5567, 19.9330,
         23.8300, 22.1611, 21.7711, 18.8249, 20.2770, 18.5243, 24.6098, 20.4580,
         21.4419, 19.0256],
        [20.6249, 21.1264, 24.0079, 22.0374, 21.9666, 19.5698, 20.1887, 19.4662,
         19.5357, 21.5444, 21.2565, 22.3177, 23.9143, 21.9929, 26.5467, 29.7345,
         20.6668, 22.3269, 20.0207, 18.0005, 19.8312, 27.4943, 21.6250, 22.8392,
         19.3403, 23.6542],
        [20.0946, 23.9340, 26.8329, 25.1565, 21.8374, 22.4192, 26.2947, 24.0052,
         21.9755, 24.6535, 30.3549, 25.3000, 19.8304, 21.8082, 18.5225, 20.1425,
         26.6529, 23.9193, 26.3632, 34.6642, 31.3424, 21.4616, 26.4799, 23.7966,
         22.3963, 20.1823],
        [19.6266, 25.4613, 26.5203, 24.5516, 25.7297, 23.0190, 24.0718, 26.4094,
         20.1950, 26.9958, 25.8544, 25.2270, 23.0218, 26.9692, 20.0886, 21.6699,
         31.1833, 28.2786, 28.3020, 28.3845, 26.0588, 20.8984, 32.4336, 27.5113,
         24.3430, 25.6188],
        [19.1124, 24.6042, 20.7116, 30.6182, 20.2238, 22.4999, 20.1572, 26.3254,
         16.7452, 21.4110, 18.6524, 19.4930, 18.0304, 19.7307, 17.0577, 21.1064,
         25.8371, 24.2935, 20.3411, 20.0508, 18.5351, 18.3101, 22.1093, 22.0051,
         25.2458, 20.2722],
        [19.7439, 18.3022, 19.4523, 17.9609, 15.5357, 18.0798, 17.9128, 19.0637,
         18.9438, 16.1648, 17.4393, 18.1843, 26.0896, 18.2333, 18.0384, 24.7725,
         16.5632, 23.6820, 16.6482, 14.9936, 20.1010, 25.6923, 17.9701, 21.2314,
         20.7283, 22.8360],
        [19.1626, 26.2659, 21.1078, 24.7023, 20.9878, 21.7831, 21.6766, 22.7311,
         13.9120, 23.6693, 22.5151, 22.1125, 20.9871, 21.9468, 18.2056, 20.6735,
         30.5986, 22.9176, 21.7996, 19.7230, 19.4991, 19.2403, 26.5106, 20.6378,
         19.2482, 21.5492]], device='cuda:0'), None], [None, None]), labels = tensor([ 0, 10, 19, 13,  5, 12, 11, 15,  8,  2,  4, 16,  6,  8, 17, 22,  9,  7,
         9, 14,  3, 18,  0,  2,  3, 13,  1,  6, 10,  7], device='cuda:0'), video_idx = tensor([  44,  967, 1782, 1232,  477, 1118, 1046, 1367,  803,  240,  412, 1509,
         550,  744, 1616, 2055,  829,  630,  873, 1302,  270, 1651,   56,  219,
         318, 1219,  163,  620,  927,  678], device='cuda:0')
